{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ati580_ml.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMMfpc4dvOwE/oTg5+bwJaM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uninstallit/ati580_final_project/blob/edvin-1/ati580_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRsRJpBpCyr3",
        "colab_type": "text"
      },
      "source": [
        "**Machine Learning portion of ATI580 Final Project**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXeaEC9UDlom",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install dnspython"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iFcX8e-rMux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install --upgrade tf-nightly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBeORpNUEj3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pymongo import MongoClient\n",
        "\n",
        "class Connect(object):\n",
        "\n",
        "  @staticmethod    \n",
        "  def get_connection(database):\n",
        "    username = \"mdbUser\"\n",
        "    password = \"ati580\"\n",
        "    return MongoClient('mongodb+srv://{}:{}@ati580-cluster.s5t5z.gcp.mongodb.net/{}?retryWrites=true&w=majority'.format(username, password, database))"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPXcZjGOa-id",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # save\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "# pip freeze --local > /content/gdrive/My\\ Drive/colab_installed.txt\n",
        "\n",
        "# # restore\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "# pip install --upgrade --force-reinstall `cat/content/gdrive/My\\ Drive/colab_installed.txt`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnUnpj-YbfmI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#plotly: https://plotly.com/python/ipython-notebook-tutorial/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b1PSfiEExeN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pymongo\n",
        "import datetime\n",
        "\n",
        "class PoliceInterviews(object):\n",
        "\n",
        "  def __init__(self, num_rows, output, eval_percent, batch):\n",
        "    # select columns\n",
        "    self._columns = dict({\n",
        "        'FieldInterviewID':0,     \n",
        "        'NOPD_Item':0,            \n",
        "        'EventDate':1,            \n",
        "        'District':1,             \n",
        "        'Zone':1,                 \n",
        "        'OfficerAssignment':0, \n",
        "        'StopDescription':1, \n",
        "        'ActionsTaken':0, \n",
        "        'VehicleYear':1, \n",
        "        'VehicleMake':1, \n",
        "        'VehicleModel':1, \n",
        "        'VehicleStyle':1, \n",
        "        'VehicleColor':1, \n",
        "        'SubjectID':0, \n",
        "        'SubjectRace':1, \n",
        "        'SubjectGender':1, \n",
        "        'SubjectAge':1, \n",
        "        'SubjectHasPhotoID':0, \n",
        "        'SubjectHeight':1, \n",
        "        'SubjectWeight':1, \n",
        "        'SubjectEyeColor':0, \n",
        "        'SubjectHairColor':1, \n",
        "        'SubjectDriverLicState':1, \n",
        "        'CreatedDateTime':0, \n",
        "        'LastModifiedDateTime':0, \n",
        "        'Longitude':0, \n",
        "        'Latitude':0, \n",
        "        'Zip':1, \n",
        "        'BlockAddress':0})\n",
        "\n",
        "    # connect to database\n",
        "    self._connect = Connect()\n",
        "    self._mdb_client = self._connect.get_connection(\"POLICE_DATABASE\")\n",
        "    self._mdb_database = self._mdb_client['POLICE_DATABASE'] \n",
        "    self._mdb_collection = self._mdb_database['POLICE_INTERVIEWS']\n",
        "    self._rows = num_rows\n",
        "\n",
        "    # data model attributes\n",
        "    self._output = output\n",
        "    self._eval_percent = eval_percent\n",
        "    self._batch = batch\n",
        "\n",
        "    # split the data into a training and validation set\n",
        "    # self._eval_df  = self._dataframe.sample(frac=self._eval_percent, random_state=123)\n",
        "    # self._train_df = self._dataframe.drop(self._eval_df.index)\n",
        "\n",
        "  # convert queries to dataframe or numpy array\n",
        "  def query_and_convert(self, filter, projection, to=\"dataframe\", rows=None):\n",
        "    if rows is not None:\n",
        "      _cursor = self._mdb_collection.find(filter, projection).limit(rows)\n",
        "    else:\n",
        "      _cursor = self._mdb_collection.find(filter, projection)\n",
        "    _dataframe = pd.DataFrame(list(_cursor))\n",
        "    if to is \"dataframe\":\n",
        "      return _dataframe\n",
        "    elif to is \"numpy\":\n",
        "      _np_array = np.transpose(np.squeeze(_dataframe.to_numpy()))\n",
        "      return _np_array\n",
        "  \n",
        "  # convert time to seconds \n",
        "  @staticmethod\n",
        "  def time_to_int_seconds(datetime_str):\n",
        "    time = datetime.datetime.strptime(datetime_str, '%m/%d/%Y %H:%M:%S %p').time()\n",
        "    [hours, minutes, seconds] = [int(t) for t in str(time).split(':')]\n",
        "    return datetime.timedelta(hours=hours, minutes=minutes, seconds=seconds).seconds\n",
        "\n",
        "  # convert license tag to binary\n",
        "  @staticmethod\n",
        "  def instate_or_outofstate(license):\n",
        "    if license is \"LA\":\n",
        "      return 1\n",
        "    return 0\n",
        "\n",
        "  # convert stop description to binary\n",
        "  @staticmethod\n",
        "  def cause_or_suspicion(description):\n",
        "    _probable_cause_list = [\"CRIMINAL VIOLATION\", \"JUVENILE VIOLATION\", \"TRAFFIC VIOLATION\"]\n",
        "    if description in _probable_cause_list:\n",
        "      return 0\n",
        "    return 1\n",
        "\n",
        "  def load_dataframe(self, split=True):\n",
        "    _filter = []\n",
        "    _projection = dict({'_id':0})\n",
        "    # create filter\n",
        "    for key, value in self._columns.items():\n",
        "      if value is 1:\n",
        "        _filter.append({key:{\"$exists\": True}})\n",
        "        _filter.append({key:{\"$ne\": \"\"}})\n",
        "        _filter.append({key:{\"$ne\": None}})\n",
        "        _filter.append({key:{\"$ne\": 0}})\n",
        "    # create projection\n",
        "    for key, value in self._columns.items():\n",
        "      if value is 1:\n",
        "        _projection[key] = value\n",
        "    # query database\n",
        "    _query_dataframe = self.query_and_convert({\"$and\":_filter}, _projection, rows=self._rows)\n",
        "    # convert specific columns\n",
        "    _query_dataframe['EventDate'] = _query_dataframe['EventDate'].apply(lambda x: self.time_to_int_seconds(x))\n",
        "    _query_dataframe['SubjectDriverLicState'] = _query_dataframe['SubjectDriverLicState'].apply(lambda x: self.instate_or_outofstate(x))\n",
        "    _query_dataframe['StopDescription'] = _query_dataframe['StopDescription'].apply(lambda x: self.cause_or_suspicion(x))\n",
        "    _query_dataframe['Zip'] = _query_dataframe['Zip'].apply(lambda x: int(x))\n",
        "    _query_dataframe['VehicleYear'] = _query_dataframe['VehicleYear'].apply(lambda x: int(x))\n",
        "    _query_dataframe['SubjectAge'] = _query_dataframe['SubjectAge'].apply(lambda x: int(x))\n",
        "    _query_dataframe['SubjectHeight'] = _query_dataframe['SubjectHeight'].apply(lambda x: int(x))\n",
        "    _query_dataframe['SubjectWeight'] = _query_dataframe['SubjectWeight'].apply(lambda x: int(x))\n",
        "    # split data into train and evaluate set\n",
        "    _eval_df = _query_dataframe.sample(frac=self._eval_percent, random_state=1234)\n",
        "    _train_df = _query_dataframe.drop(_eval_df.index)\n",
        "    if split is True:\n",
        "      # split data into train and evaluate set\n",
        "      _eval_df = _query_dataframe.sample(frac=self._eval_percent, random_state=1234)\n",
        "      _train_df = _query_dataframe.drop(_eval_df.index)\n",
        "      return _train_df, _eval_df\n",
        "    return _query_dataframe\n",
        "\n",
        "  # convert dataframe_to_dataset \n",
        "  @staticmethod\n",
        "  def dataframe_to_dataset(dataframe, output):\n",
        "    _dataframe = dataframe.copy()\n",
        "    _labels    = _dataframe.pop(output)\n",
        "    _dataset   = tf.data.Dataset.from_tensor_slices((dict(_dataframe), _labels))\n",
        "    _dataset   = _dataset.shuffle(buffer_size=len(_dataframe))\n",
        "    return _dataset\n",
        "\n",
        "  def load_dataset(self):\n",
        "    _train_df, _eval_df = self.load_dataframe()\n",
        "\n",
        "    _train_ds = self.dataframe_to_dataset(_train_df, self._output)\n",
        "    _eval_ds  = self.dataframe_to_dataset(_eval_df, self._output)\n",
        "\n",
        "    _train_ds = _train_ds.batch(self._batch)\n",
        "    _eval_ds  = _eval_ds.batch(self._batch)\n",
        "    return _train_ds, _eval_ds\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR3j6WP06CF1",
        "colab_type": "text"
      },
      "source": [
        "## **Classification with Neural Networks**\n",
        "- Source: [Keras | Code examples](https://keras.io/examples/structured_data/structured_data_classification_from_scratch/)\n",
        "- CategoricalEncoding API: [Tensorflow](https://tensorflow.google.cn/api_docs/python/tf/keras/layers/experimental/preprocessing/CategoryEncoding#methods)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjRg39ibH7yp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pull data from db and convert\n",
        "police_interviews = PoliceInterviews(num_rows=1000, output=\"StopDescription\", eval_percent=0.2, batch=25)\n",
        "train_ds, eval_ds = police_interviews.load_dataset()\n",
        "\n",
        "# visual check\n",
        "# train_df, eval_df = police_interviews.load_dataframe()\n",
        "# print('{:<25} {} \\n'.format(\"Column Name\", \"Sample\"))\n",
        "# for (column_name, column_data) in train_df.iteritems():\n",
        "#   print('{:<25} {}'.format(column_name, column_data[0]))"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC92GRZJmEUd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
        "from tensorflow.keras.layers.experimental.preprocessing import CategoryEncoding\n",
        "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "\n",
        "def encode_numerical_feature(feature, name, dataset):\n",
        "    # Create a Normalization layer for our feature\n",
        "    normalizer = Normalization()\n",
        "    # Prepare a Dataset that only yields our feature\n",
        "    feature_ds = dataset.map(lambda x, y: x[name])\n",
        "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
        "    # Learn the statistics of the data\n",
        "    normalizer.adapt(feature_ds)\n",
        "    # Normalize the input feature\n",
        "    encoded_feature = normalizer(feature)\n",
        "    return encoded_feature\n",
        "\n",
        "def encode_string_categorical_feature(feature, name, dataset):\n",
        "    # Create a StringLookup layer which will turn strings into integer indices\n",
        "    index = StringLookup()\n",
        "    # Prepare a Dataset that only yields our feature\n",
        "    feature_ds = dataset.map(lambda x, y: x[name])\n",
        "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
        "    # Learn the set of possible string values and assign them a fixed integer index\n",
        "    index.adapt(feature_ds)\n",
        "    # Turn the string input into integer indices\n",
        "    encoded_feature = index(feature)\n",
        "    # Create a CategoryEncoding for our integer indices\n",
        "    encoder = CategoryEncoding(output_mode=\"binary\")\n",
        "    # Prepare a dataset of indices\n",
        "    feature_ds = feature_ds.map(index)\n",
        "    # Learn the space of possible indices\n",
        "    encoder.adapt(feature_ds)\n",
        "    # Apply one-hot encoding to our indices\n",
        "    encoded_feature = encoder(encoded_feature)\n",
        "    return encoded_feature\n",
        "\n",
        "def encode_integer_categorical_feature(feature, name, dataset):\n",
        "    # Create a CategoryEncoding for our integer indices\n",
        "    encoder = CategoryEncoding(output_mode=\"binary\")\n",
        "    # Prepare a Dataset that only yields our feature\n",
        "    feature_ds = dataset.map(lambda x, y: x[name])\n",
        "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
        "    # Learn the space of possible indices\n",
        "    encoder.adapt(feature_ds)\n",
        "    # Apply one-hot encoding to our indices\n",
        "    encoded_feature = encoder(feature)\n",
        "    return encoded_feature\n",
        "\n",
        "# categorical features encoded as integers\n",
        "inout_state  = keras.Input(shape=(1,), name=\"SubjectDriverLicState\", dtype=\"int64\")\n",
        "district     = keras.Input(shape=(1,), name=\"District\", dtype=\"int64\")\n",
        "vehicle_year = keras.Input(shape=(1,), name=\"VehicleYear\", dtype=\"int64\")\n",
        "zip          = keras.Input(shape=(1,), name=\"Zip\", dtype=\"int64\")\n",
        "# output\n",
        "# stop_description = keras.Input(shape=(1,), name=\"StopDescription\", dtype=\"int64\")\n",
        "\n",
        "# categorical feature encoded as string\n",
        "zone             = keras.Input(shape=(1,), name=\"Zone\", dtype=\"string\")\n",
        "vehicle_make     = keras.Input(shape=(1,), name=\"VehicleMake\", dtype=\"string\")\n",
        "vehicle_model    = keras.Input(shape=(1,), name=\"VehicleModel\", dtype=\"string\")\n",
        "vehicle_style    = keras.Input(shape=(1,), name=\"VehicleStyle\", dtype=\"string\")\n",
        "vehicle_color    = keras.Input(shape=(1,), name=\"VehicleColor\", dtype=\"string\")\n",
        "subject_race     = keras.Input(shape=(1,), name=\"SubjectRace\", dtype=\"string\")\n",
        "subject_gender   = keras.Input(shape=(1,), name=\"SubjectGender\", dtype=\"string\")\n",
        "hair_color       = keras.Input(shape=(1,), name=\"SubjectHairColor\", dtype=\"string\")\n",
        "\n",
        "# numerical features\n",
        "time           = keras.Input(shape=(1,), name=\"EventDate\")\n",
        "subject_age    = keras.Input(shape=(1,), name=\"SubjectAge\")\n",
        "subject_height = keras.Input(shape=(1,), name=\"SubjectHeight\")\n",
        "subject_weight = keras.Input(shape=(1,), name=\"SubjectWeight\")\n",
        "\n",
        "all_inputs = [inout_state,\n",
        "              district, \n",
        "              vehicle_year, \n",
        "              zip, \n",
        "              zone, \n",
        "              vehicle_make, \n",
        "              vehicle_model, \n",
        "              vehicle_style, \n",
        "              vehicle_color, \n",
        "              subject_race, \n",
        "              subject_gender, \n",
        "              hair_color, \n",
        "              time, \n",
        "              subject_age, \n",
        "              subject_height, \n",
        "              subject_weight]\n",
        "\n",
        "# integer categorical features\n",
        "inout_state_encoded  = encode_integer_categorical_feature(inout_state, \"SubjectDriverLicState\", train_ds)\n",
        "district_encoded     = encode_integer_categorical_feature(district, \"District\", train_ds)\n",
        "vehicle_year_encoded = encode_integer_categorical_feature(vehicle_year, \"VehicleYear\", train_ds)\n",
        "zip_encoded          = encode_integer_categorical_feature(zip, \"Zip\", train_ds)\n",
        "\n",
        "# string categorical features\n",
        "zone_encoded             = encode_string_categorical_feature(zone, \"Zone\", train_ds)\n",
        "vehicle_make_encoded     = encode_string_categorical_feature(vehicle_make, \"VehicleMake\", train_ds)\n",
        "vehicle_model_encoded    = encode_string_categorical_feature(vehicle_model, \"VehicleModel\", train_ds)\n",
        "vehicle_style_encoded    = encode_string_categorical_feature(vehicle_style, \"VehicleStyle\", train_ds)\n",
        "vehicle_color_encoded    = encode_string_categorical_feature(vehicle_color, \"VehicleColor\", train_ds)\n",
        "subject_race_encoded     = encode_string_categorical_feature(subject_race, \"SubjectRace\", train_ds)\n",
        "subject_gender_encoded   = encode_string_categorical_feature(subject_gender, \"SubjectGender\", train_ds)\n",
        "hair_color_encoded       = encode_string_categorical_feature(hair_color, \"SubjectHairColor\", train_ds)\n",
        "\n",
        "# numerical features\n",
        "time_encoded           = encode_numerical_feature(time, \"EventDate\", train_ds)\n",
        "subject_age_encoded    = encode_numerical_feature(subject_age, \"SubjectAge\", train_ds)\n",
        "subject_height_encoded = encode_numerical_feature(subject_height, \"SubjectHeight\", train_ds)\n",
        "subject_weight_encoded = encode_numerical_feature(subject_weight, \"SubjectWeight\", train_ds)\n",
        "\n",
        "all_features = layers.concatenate([\n",
        "                                   inout_state_encoded,\n",
        "                                   district_encoded,\n",
        "                                   vehicle_year_encoded,\n",
        "                                   zip_encoded,\n",
        "                                   zone_encoded,\n",
        "                                   vehicle_make_encoded,\n",
        "                                   vehicle_model_encoded,\n",
        "                                   vehicle_style_encoded,\n",
        "                                   vehicle_color_encoded,\n",
        "                                   subject_race_encoded,\n",
        "                                   subject_gender_encoded,\n",
        "                                   hair_color_encoded,\n",
        "                                   time_encoded,\n",
        "                                   subject_age_encoded,\n",
        "                                   subject_height_encoded,\n",
        "                                   subject_weight_encoded])\n",
        "\n",
        "# build model\n",
        "x = layers.Dense(32, activation=\"relu\")(all_features)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "x = layers.Dense(16, activation=\"sigmoid\")(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "x = layers.Dense(8, activation=\"sigmoid\")(x)\n",
        "output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(all_inputs, output)\n",
        "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9BjVItnr9Gy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model diagram\n",
        "keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_geQsn5ssog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train model\n",
        "model.fit(train_ds, epochs=5, validation_data=eval_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxGLH6CmHTC2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predict from model\n",
        "sample = dict({\n",
        "    \"SubjectDriverLicState\":0,\n",
        "    \"District\":7,\n",
        "    \"VehicleYear\":1997,\n",
        "    \"Zip\":70127,\n",
        "    \"Zone\":\"O\",\n",
        "    \"VehicleMake\":\"CHEVROLET\",\n",
        "    \"VehicleModel\":\"TAHOE\",\n",
        "    \"VehicleStyle\":\"SPORTS UTILITY\",\n",
        "    \"VehicleColor\":\"BLUE\",\n",
        "    \"SubjectRace\":\"BLACK\",\n",
        "    \"SubjectGender\":\"MALE\",\n",
        "    \"SubjectHairColor\":\"Black\",\n",
        "    \"EventDate\":34500,\n",
        "    \"SubjectAge\":21,\n",
        "    \"SubjectHeight\":72,\n",
        "    \"SubjectWeight\":169,\n",
        "})\n",
        "\n",
        "input_dict = {name: tf.convert_to_tensor([value]) for name, value in sample.items()}\n",
        "tf.print(input_dict)\n",
        "model.predict(input_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdB5Xh2Q9dc9",
        "colab_type": "text"
      },
      "source": [
        "## **Logistic Regression**\n",
        "\n",
        "Binary Logistic Regression, in which the target variable has only two possible values, e.g., pass/fail or win/lose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7N_QcET9cH6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "eaff122b-2141-40ad-9383-91910bfc806f"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import OneHotEncoder, normalize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def encode_cat_features(features, dataframe, encoder):\n",
        "  _dataframe = dataframe.copy()\n",
        "  _dataframe = dataframe[features]\n",
        "  transform_array = encoder.transform(_dataframe).toarray()\n",
        "  return np.transpose(transform_array)\n",
        "\n",
        "def z_score_norm(x, mean, std):\n",
        "  return ( float(x) - float(mean) ) / float(std)\n",
        "\n",
        "def inv_z_score_norm(z, mean, std):\n",
        "  return float(mean) + float(z) * float(std)\n",
        "\n",
        "def encode_norm_features(features, dataframe, mean=None, std=None):\n",
        "  _dataframe = dataframe.copy()\n",
        "  _dataframe = _dataframe[features]\n",
        "  for feature in features:\n",
        "    _mean = mean[feature]\n",
        "    _std  = std[feature]\n",
        "    _dataframe[feature] = _dataframe[feature].apply(lambda x: z_score_norm(x, _mean, _std))\n",
        "  return np.transpose(_dataframe.to_numpy())\n",
        "  \n",
        "# load data\n",
        "police_df = police_interviews.load_dataframe(split=False)\n",
        "police_label_df = police_df.pop(\"StopDescription\")\n",
        "\n",
        "# categorical features\n",
        "categorical = [\"Zone\", \"VehicleMake\", \"VehicleModel\", \"VehicleStyle\",             # string\n",
        "               \"VehicleColor\", \"SubjectRace\", \"SubjectGender\",\"SubjectHairColor\", # string\n",
        "               \"SubjectDriverLicState\", \"District\", \"VehicleYear\", \"Zip\"]         # int\n",
        "\n",
        "numerical = [\"EventDate\", \"SubjectAge\", \"SubjectHeight\", \"SubjectWeight\"]\n",
        "\n",
        "# one-hot encoder\n",
        "# train on the whole set to avoid feature dim errors\n",
        "onehot_encoder = OneHotEncoder(sparse=True)\n",
        "onehot_encoder.fit(police_df[categorical])\n",
        "\n",
        "# split the data into train and test\n",
        "x_train, x_test, y_train, y_test = train_test_split(police_df, police_label_df, test_size=0.2, random_state=1234)\n",
        "\n",
        "# categorical features encoding\n",
        "x_train_cat_encoded = encode_cat_features(categorical, x_train, onehot_encoder)\n",
        "x_test_cat_encoded  = encode_cat_features(categorical, x_test, onehot_encoder)\n",
        "\n",
        "# # numerical features encoding\n",
        "mean = x_train[numerical].mean(axis=0)\n",
        "std  = x_train[numerical].std(axis=0)\n",
        "x_train_num_encoded = encode_norm_features(numerical, x_train, mean, std)\n",
        "x_test_num_encoded  = encode_norm_features(numerical, x_test, mean, std)\n",
        "\n",
        "# # combine categorical and numerical\n",
        "x_train_combined = np.vstack([x_train_cat_encoded, x_train_num_encoded])\n",
        "x_train_combined = np.transpose(x_train_combined)\n",
        "x_test_combined = np.vstack([x_test_cat_encoded, x_test_num_encoded])\n",
        "x_test_combined = np.transpose(x_test_combined)\n",
        "\n",
        "# train model\n",
        "logistic_regression = LogisticRegression(random_state=0).fit(x_train_combined, y_train)\n",
        "\n",
        "# # accuracy on test set\n",
        "print(logistic_regression.score(x_train_combined, y_train))\n",
        "print(logistic_regression.score(x_test_combined, y_test))\n",
        "\n",
        "# # predict classes\n",
        "predicted = logistic_regression.predict(x_test_combined[:200, :])\n",
        "expected = y_test[:200].to_numpy()\n",
        "\n",
        "# predict probability\n",
        "logistic_regression.predict_proba(x_test_combined[:200, :])\n",
        "\n",
        "# confusion matrix\n",
        "tn, fp, fn, tp = confusion_matrix(expected, predicted, normalize='pred').ravel()\n",
        "\n",
        "print(\"True Negative:  \", tn) \n",
        "print(\"False Positive: \", fp)\n",
        "print(\"False Negative: \", fn)\n",
        "print(\"True Prositive: \", tp)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.88\n",
            "0.83\n",
            "True Negative:   0.8429319371727748\n",
            "False Positive:  0.4444444444444444\n",
            "False Negative:  0.15706806282722513\n",
            "True Prositive:  0.5555555555555556\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}