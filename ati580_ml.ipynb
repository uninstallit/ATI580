{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ati580_ml.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMU6TPFDw/mKU5JvBB8ywkC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uninstallit/ati580_final_project/blob/edvin-1/ati580_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRsRJpBpCyr3",
        "colab_type": "text"
      },
      "source": [
        "**Machine Learning portion of ATI580 Final Project**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXeaEC9UDlom",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "a316cf72-fc6e-4863-d4d5-68091e641d48"
      },
      "source": [
        "pip install dnspython"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting dnspython\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/d3/3aa0e7213ef72b8585747aa0e271a9523e713813b9a20177ebe1e939deb0/dnspython-1.16.0-py2.py3-none-any.whl (188kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 2.6MB/s \n",
            "\u001b[?25hInstalling collected packages: dnspython\n",
            "Successfully installed dnspython-1.16.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iFcX8e-rMux",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 904
        },
        "outputId": "dab0eeda-b0e7-4c7a-9d6b-8af11444e5e6"
      },
      "source": [
        "pip install --upgrade tf-nightly"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/9f/e4f87927ea5d63063f1755c929f3a93112e686d1166ea8a01c3b4f0181ed/tf_nightly-2.4.0.dev20200712-cp36-cp36m-manylinux2010_x86_64.whl (322.7MB)\n",
            "\u001b[K     |████████████████████████████████| 322.7MB 50kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.34.2)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.30.0)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.3.3)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: scipy==1.4.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.6.3)\n",
            "Collecting tf-estimator-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/34/0f5a56250d59995ba3a6cf9a5946bb8201cb7e160a517d296579d7eeb421/tf_estimator_nightly-2.4.0.dev2020071201-py2.py3-none-any.whl (459kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 47.7MB/s \n",
            "\u001b[?25hCollecting tb-nightly<2.4.0a0,>=2.3.0a0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/60/76/b51c683c6a7cb0c76e98ea82fd3999a896680eab54adaeef0cfc52a432d7/tb_nightly-2.3.0a20200712-py3-none-any.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8MB 48.0MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tf-nightly) (47.3.1)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.6.0.post3)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.17.2)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.6.1)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (4.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (4.6)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (2.9)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.1.0)\n",
            "Installing collected packages: tf-estimator-nightly, tb-nightly, tf-nightly\n",
            "Successfully installed tb-nightly-2.3.0a20200712 tf-estimator-nightly-2.4.0.dev2020071201 tf-nightly-2.4.0.dev20200712\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBeORpNUEj3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pymongo import MongoClient\n",
        "\n",
        "class Connect(object):\n",
        "\n",
        "  @staticmethod    \n",
        "  def get_connection(database):\n",
        "    username = \"mdbUser\"\n",
        "    password = \"ati580\"\n",
        "    return MongoClient('mongodb+srv://{}:{}@ati580-cluster.s5t5z.gcp.mongodb.net/{}?retryWrites=true&w=majority'.format(username, password, database))"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPXcZjGOa-id",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # save\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "# pip freeze --local > /content/gdrive/My\\ Drive/colab_installed.txt\n",
        "\n",
        "# # restore\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "# pip install --upgrade --force-reinstall `cat/content/gdrive/My\\ Drive/colab_installed.txt`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnUnpj-YbfmI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#plotly: https://plotly.com/python/ipython-notebook-tutorial/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b1PSfiEExeN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pymongo\n",
        "import datetime\n",
        "\n",
        "class PoliceInterviews(object):\n",
        "\n",
        "  def __init__(self, num_rows, output, eval_percent, batch):\n",
        "    # select columns\n",
        "    self._columns = dict({\n",
        "        'FieldInterviewID':0,     \n",
        "        'NOPD_Item':0,            \n",
        "        'EventDate':1,            \n",
        "        'District':1,             \n",
        "        'Zone':1,                 \n",
        "        'OfficerAssignment':0, \n",
        "        'StopDescription':1, \n",
        "        'ActionsTaken':0, \n",
        "        'VehicleYear':1, \n",
        "        'VehicleMake':1, \n",
        "        'VehicleModel':1, \n",
        "        'VehicleStyle':1, \n",
        "        'VehicleColor':1, \n",
        "        'SubjectID':0, \n",
        "        'SubjectRace':1, \n",
        "        'SubjectGender':1, \n",
        "        'SubjectAge':1, \n",
        "        'SubjectHasPhotoID':0, \n",
        "        'SubjectHeight':1, \n",
        "        'SubjectWeight':1, \n",
        "        'SubjectEyeColor':0, \n",
        "        'SubjectHairColor':1, \n",
        "        'SubjectDriverLicState':1, \n",
        "        'CreatedDateTime':0, \n",
        "        'LastModifiedDateTime':0, \n",
        "        'Longitude':0, \n",
        "        'Latitude':0, \n",
        "        'Zip':1, \n",
        "        'BlockAddress':0})\n",
        "\n",
        "    # connect to database\n",
        "    self._connect = Connect()\n",
        "    self._mdb_client = self._connect.get_connection(\"POLICE_DATABASE\")\n",
        "    self._mdb_database = self._mdb_client['POLICE_DATABASE'] \n",
        "    self._mdb_collection = self._mdb_database['POLICE_INTERVIEWS']\n",
        "    self._rows = num_rows\n",
        "\n",
        "    # data model attributes\n",
        "    self._output = output\n",
        "    self._eval_percent = eval_percent\n",
        "    self._batch = batch\n",
        "\n",
        "    # split the data into a training and validation set\n",
        "    # self._eval_df  = self._dataframe.sample(frac=self._eval_percent, random_state=123)\n",
        "    # self._train_df = self._dataframe.drop(self._eval_df.index)\n",
        "\n",
        "  # convert queries to dataframe or numpy array\n",
        "  def query_and_convert(self, filter, projection, to=\"dataframe\", rows=None):\n",
        "    if rows is not None:\n",
        "      _cursor = self._mdb_collection.find(filter, projection).limit(rows)\n",
        "    else:\n",
        "      _cursor = self._mdb_collection.find(filter, projection)\n",
        "    _dataframe = pd.DataFrame(list(_cursor))\n",
        "    if to is \"dataframe\":\n",
        "      return _dataframe\n",
        "    elif to is \"numpy\":\n",
        "      _np_array = np.transpose(np.squeeze(_dataframe.to_numpy()))\n",
        "      return _np_array\n",
        "  \n",
        "  # convert time to seconds \n",
        "  @staticmethod\n",
        "  def time_to_int_seconds(datetime_str):\n",
        "    time = datetime.datetime.strptime(datetime_str, '%m/%d/%Y %H:%M:%S %p').time()\n",
        "    [hours, minutes, seconds] = [int(t) for t in str(time).split(':')]\n",
        "    return datetime.timedelta(hours=hours, minutes=minutes, seconds=seconds).seconds\n",
        "\n",
        "  # convert license tag to binary\n",
        "  @staticmethod\n",
        "  def instate_or_outofstate(license):\n",
        "    if license is \"LA\":\n",
        "      return 1\n",
        "    return 0\n",
        "\n",
        "  # convert stop description to binary\n",
        "  @staticmethod\n",
        "  def cause_or_suspicion(description):\n",
        "    _probable_cause_list = [\"CRIMINAL VIOLATION\", \"JUVENILE VIOLATION\", \"TRAFFIC VIOLATION\"]\n",
        "    if description in _probable_cause_list:\n",
        "      return 0\n",
        "    return 1\n",
        "\n",
        "  def load_dataframe(self, split=True):\n",
        "    _filter = []\n",
        "    _projection = dict({'_id':0})\n",
        "    # create filter\n",
        "    for key, value in self._columns.items():\n",
        "      if value is 1:\n",
        "        _filter.append({key:{\"$exists\": True}})\n",
        "        _filter.append({key:{\"$ne\": \"\"}})\n",
        "        _filter.append({key:{\"$ne\": None}})\n",
        "        _filter.append({key:{\"$ne\": 0}})\n",
        "    # create projection\n",
        "    for key, value in self._columns.items():\n",
        "      if value is 1:\n",
        "        _projection[key] = value\n",
        "    # query database\n",
        "    _query_dataframe = self.query_and_convert({\"$and\":_filter}, _projection, rows=self._rows)\n",
        "    # convert specific columns\n",
        "    _query_dataframe['EventDate'] = _query_dataframe['EventDate'].apply(lambda x: self.time_to_int_seconds(x))\n",
        "    _query_dataframe['SubjectDriverLicState'] = _query_dataframe['SubjectDriverLicState'].apply(lambda x: self.instate_or_outofstate(x))\n",
        "    _query_dataframe['StopDescription'] = _query_dataframe['StopDescription'].apply(lambda x: self.cause_or_suspicion(x))\n",
        "    _query_dataframe['Zip'] = _query_dataframe['Zip'].apply(lambda x: int(x))\n",
        "    _query_dataframe['VehicleYear'] = _query_dataframe['VehicleYear'].apply(lambda x: int(x))\n",
        "    _query_dataframe['SubjectAge'] = _query_dataframe['SubjectAge'].apply(lambda x: int(x))\n",
        "    _query_dataframe['SubjectHeight'] = _query_dataframe['SubjectHeight'].apply(lambda x: int(x))\n",
        "    _query_dataframe['SubjectWeight'] = _query_dataframe['SubjectWeight'].apply(lambda x: int(x))\n",
        "    # split data into train and evaluate set\n",
        "    _eval_df = _query_dataframe.sample(frac=self._eval_percent, random_state=1234)\n",
        "    _train_df = _query_dataframe.drop(_eval_df.index)\n",
        "    if split is False:\n",
        "      return _query_dataframe\n",
        "    return _train_df, _eval_df\n",
        "\n",
        "  # convert dataframe_to_dataset \n",
        "  @staticmethod\n",
        "  def dataframe_to_dataset(dataframe, output):\n",
        "    _dataframe = dataframe.copy()\n",
        "    _labels    = _dataframe.pop(output)\n",
        "    _dataset   = tf.data.Dataset.from_tensor_slices((dict(_dataframe), _labels))\n",
        "    _dataset   = _dataset.shuffle(buffer_size=len(_dataframe))\n",
        "    return _dataset\n",
        "\n",
        "  def load_dataset(self):\n",
        "    _train_df, _eval_df = self.load_dataframe()\n",
        "\n",
        "    _train_ds = self.dataframe_to_dataset(_train_df, self._output)\n",
        "    _eval_ds  = self.dataframe_to_dataset(_eval_df, self._output)\n",
        "\n",
        "    _train_ds = _train_ds.batch(self._batch)\n",
        "    _eval_ds  = _eval_ds.batch(self._batch)\n",
        "    return _train_ds, _eval_ds\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR3j6WP06CF1",
        "colab_type": "text"
      },
      "source": [
        "## **Classification with Neural Networks**\n",
        "- Source: [Keras | Code examples](https://keras.io/examples/structured_data/structured_data_classification_from_scratch/)\n",
        "- CategoricalEncoding API: [Tensorflow](https://tensorflow.google.cn/api_docs/python/tf/keras/layers/experimental/preprocessing/CategoryEncoding#methods)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjRg39ibH7yp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "outputId": "b7a95f87-7e2e-4015-a6c6-8e53ff1b0dfa"
      },
      "source": [
        "# pull data from db and convert\n",
        "police_interviews = PoliceInterviews(num_rows=1000, output=\"StopDescription\", eval_percent=0.2, batch=25)\n",
        "train_ds, eval_ds = police_interviews.load_dataset()\n",
        "\n",
        "# visual check\n",
        "# train_df, eval_df = police_interviews.load_dataframe()\n",
        "# print('{:<25} {} \\n'.format(\"Column Name\", \"Sample\"))\n",
        "# for (column_name, column_data) in train_df.iteritems():\n",
        "#   print('{:<25} {}'.format(column_name, column_data[0]))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['EventDate', 'District', 'Zone', 'StopDescription', 'VehicleYear',\n",
            "       'VehicleMake', 'VehicleModel', 'VehicleStyle', 'VehicleColor',\n",
            "       'SubjectRace', 'SubjectGender', 'SubjectAge', 'SubjectHeight',\n",
            "       'SubjectWeight', 'SubjectHairColor', 'SubjectDriverLicState', 'Zip'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC92GRZJmEUd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
        "from tensorflow.keras.layers.experimental.preprocessing import CategoryEncoding\n",
        "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "\n",
        "def encode_numerical_feature(feature, name, dataset):\n",
        "    # Create a Normalization layer for our feature\n",
        "    normalizer = Normalization()\n",
        "    # Prepare a Dataset that only yields our feature\n",
        "    feature_ds = dataset.map(lambda x, y: x[name])\n",
        "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
        "    # Learn the statistics of the data\n",
        "    normalizer.adapt(feature_ds)\n",
        "    # Normalize the input feature\n",
        "    encoded_feature = normalizer(feature)\n",
        "    return encoded_feature\n",
        "\n",
        "def encode_string_categorical_feature(feature, name, dataset):\n",
        "    # Create a StringLookup layer which will turn strings into integer indices\n",
        "    index = StringLookup()\n",
        "    # Prepare a Dataset that only yields our feature\n",
        "    feature_ds = dataset.map(lambda x, y: x[name])\n",
        "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
        "    # Learn the set of possible string values and assign them a fixed integer index\n",
        "    index.adapt(feature_ds)\n",
        "    # Turn the string input into integer indices\n",
        "    encoded_feature = index(feature)\n",
        "    # Create a CategoryEncoding for our integer indices\n",
        "    encoder = CategoryEncoding(output_mode=\"binary\")\n",
        "    # Prepare a dataset of indices\n",
        "    feature_ds = feature_ds.map(index)\n",
        "    # Learn the space of possible indices\n",
        "    encoder.adapt(feature_ds)\n",
        "    # Apply one-hot encoding to our indices\n",
        "    encoded_feature = encoder(encoded_feature)\n",
        "    return encoded_feature\n",
        "\n",
        "def encode_integer_categorical_feature(feature, name, dataset):\n",
        "    # Create a CategoryEncoding for our integer indices\n",
        "    encoder = CategoryEncoding(output_mode=\"binary\")\n",
        "    # Prepare a Dataset that only yields our feature\n",
        "    feature_ds = dataset.map(lambda x, y: x[name])\n",
        "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
        "    # Learn the space of possible indices\n",
        "    encoder.adapt(feature_ds)\n",
        "    # Apply one-hot encoding to our indices\n",
        "    encoded_feature = encoder(feature)\n",
        "    return encoded_feature\n",
        "\n",
        "# categorical features encoded as integers\n",
        "inout_state  = keras.Input(shape=(1,), name=\"SubjectDriverLicState\", dtype=\"int64\")\n",
        "district     = keras.Input(shape=(1,), name=\"District\", dtype=\"int64\")\n",
        "vehicle_year = keras.Input(shape=(1,), name=\"VehicleYear\", dtype=\"int64\")\n",
        "zip          = keras.Input(shape=(1,), name=\"Zip\", dtype=\"int64\")\n",
        "# output\n",
        "# stop_description = keras.Input(shape=(1,), name=\"StopDescription\", dtype=\"int64\")\n",
        "\n",
        "# categorical feature encoded as string\n",
        "zone             = keras.Input(shape=(1,), name=\"Zone\", dtype=\"string\")\n",
        "vehicle_make     = keras.Input(shape=(1,), name=\"VehicleMake\", dtype=\"string\")\n",
        "vehicle_model    = keras.Input(shape=(1,), name=\"VehicleModel\", dtype=\"string\")\n",
        "vehicle_style    = keras.Input(shape=(1,), name=\"VehicleStyle\", dtype=\"string\")\n",
        "vehicle_color    = keras.Input(shape=(1,), name=\"VehicleColor\", dtype=\"string\")\n",
        "subject_race     = keras.Input(shape=(1,), name=\"SubjectRace\", dtype=\"string\")\n",
        "subject_gender   = keras.Input(shape=(1,), name=\"SubjectGender\", dtype=\"string\")\n",
        "hair_color       = keras.Input(shape=(1,), name=\"SubjectHairColor\", dtype=\"string\")\n",
        "\n",
        "# numerical features\n",
        "time           = keras.Input(shape=(1,), name=\"EventDate\")\n",
        "subject_age    = keras.Input(shape=(1,), name=\"SubjectAge\")\n",
        "subject_height = keras.Input(shape=(1,), name=\"SubjectHeight\")\n",
        "subject_weight = keras.Input(shape=(1,), name=\"SubjectWeight\")\n",
        "\n",
        "all_inputs = [inout_state,\n",
        "              district, \n",
        "              vehicle_year, \n",
        "              zip, \n",
        "              zone, \n",
        "              vehicle_make, \n",
        "              vehicle_model, \n",
        "              vehicle_style, \n",
        "              vehicle_color, \n",
        "              subject_race, \n",
        "              subject_gender, \n",
        "              hair_color, \n",
        "              time, \n",
        "              subject_age, \n",
        "              subject_height, \n",
        "              subject_weight]\n",
        "\n",
        "# integer categorical features\n",
        "inout_state_encoded  = encode_integer_categorical_feature(inout_state, \"SubjectDriverLicState\", train_ds)\n",
        "district_encoded     = encode_integer_categorical_feature(district, \"District\", train_ds)\n",
        "vehicle_year_encoded = encode_integer_categorical_feature(vehicle_year, \"VehicleYear\", train_ds)\n",
        "zip_encoded          = encode_integer_categorical_feature(zip, \"Zip\", train_ds)\n",
        "\n",
        "# string categorical features\n",
        "zone_encoded             = encode_string_categorical_feature(zone, \"Zone\", train_ds)\n",
        "vehicle_make_encoded     = encode_string_categorical_feature(vehicle_make, \"VehicleMake\", train_ds)\n",
        "vehicle_model_encoded    = encode_string_categorical_feature(vehicle_model, \"VehicleModel\", train_ds)\n",
        "vehicle_style_encoded    = encode_string_categorical_feature(vehicle_style, \"VehicleStyle\", train_ds)\n",
        "vehicle_color_encoded    = encode_string_categorical_feature(vehicle_color, \"VehicleColor\", train_ds)\n",
        "subject_race_encoded     = encode_string_categorical_feature(subject_race, \"SubjectRace\", train_ds)\n",
        "subject_gender_encoded   = encode_string_categorical_feature(subject_gender, \"SubjectGender\", train_ds)\n",
        "hair_color_encoded       = encode_string_categorical_feature(hair_color, \"SubjectHairColor\", train_ds)\n",
        "\n",
        "# numerical features\n",
        "time_encoded           = encode_numerical_feature(time, \"EventDate\", train_ds)\n",
        "subject_age_encoded    = encode_numerical_feature(subject_age, \"SubjectAge\", train_ds)\n",
        "subject_height_encoded = encode_numerical_feature(subject_height, \"SubjectHeight\", train_ds)\n",
        "subject_weight_encoded = encode_numerical_feature(subject_weight, \"SubjectWeight\", train_ds)\n",
        "\n",
        "all_features = layers.concatenate([\n",
        "                                   inout_state_encoded,\n",
        "                                   district_encoded,\n",
        "                                   vehicle_year_encoded,\n",
        "                                   zip_encoded,\n",
        "                                   zone_encoded,\n",
        "                                   vehicle_make_encoded,\n",
        "                                   vehicle_model_encoded,\n",
        "                                   vehicle_style_encoded,\n",
        "                                   vehicle_color_encoded,\n",
        "                                   subject_race_encoded,\n",
        "                                   subject_gender_encoded,\n",
        "                                   hair_color_encoded,\n",
        "                                   time_encoded,\n",
        "                                   subject_age_encoded,\n",
        "                                   subject_height_encoded,\n",
        "                                   subject_weight_encoded])\n",
        "\n",
        "# build model\n",
        "x = layers.Dense(32, activation=\"relu\")(all_features)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "x = layers.Dense(16, activation=\"sigmoid\")(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "x = layers.Dense(8, activation=\"sigmoid\")(x)\n",
        "output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(all_inputs, output)\n",
        "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9BjVItnr9Gy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model diagram\n",
        "keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_geQsn5ssog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train model\n",
        "model.fit(train_ds, epochs=5, validation_data=eval_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxGLH6CmHTC2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predict from model\n",
        "sample = dict({\n",
        "    \"SubjectDriverLicState\":0,\n",
        "    \"District\":7,\n",
        "    \"VehicleYear\":1997,\n",
        "    \"Zip\":70127,\n",
        "    \"Zone\":\"O\",\n",
        "    \"VehicleMake\":\"CHEVROLET\",\n",
        "    \"VehicleModel\":\"TAHOE\",\n",
        "    \"VehicleStyle\":\"SPORTS UTILITY\",\n",
        "    \"VehicleColor\":\"BLUE\",\n",
        "    \"SubjectRace\":\"BLACK\",\n",
        "    \"SubjectGender\":\"MALE\",\n",
        "    \"SubjectHairColor\":\"Black\",\n",
        "    \"EventDate\":34500,\n",
        "    \"SubjectAge\":21,\n",
        "    \"SubjectHeight\":72,\n",
        "    \"SubjectWeight\":169,\n",
        "})\n",
        "\n",
        "input_dict = {name: tf.convert_to_tensor([value]) for name, value in sample.items()}\n",
        "tf.print(input_dict)\n",
        "model.predict(input_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdB5Xh2Q9dc9",
        "colab_type": "text"
      },
      "source": [
        "## **Logistic Regression**\n",
        "\n",
        "Binary Logistic Regression, in which the target variable has only two possible values, e.g., pass/fail or win/lose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7N_QcET9cH6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        },
        "outputId": "bd91ca51-6fc0-41ea-b971-2a66a702d789"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# load data\n",
        "police_df = police_interviews.load_dataframe(split=False)\n",
        "\n",
        "# separate target column\n",
        "police_label_array = police_df.pop(\"StopDescription\").to_numpy()\n",
        "\n",
        "# categorical features\n",
        "categorical_string = [\"Zone\", \"VehicleMake\", \"VehicleModel\", \"VehicleStyle\",             # string\n",
        "                      \"VehicleColor\", \"SubjectRace\", \"SubjectGender\",\"SubjectHairColor\", # string\n",
        "                      \"SubjectDriverLicState\", \"District\", \"VehicleYear\", \"Zip\"]         # int\n",
        "police_cat_df = police_df[categorical_string]\n",
        "one_hot = preprocessing.OneHotEncoder(sparse=True)\n",
        "one_hot.fit(police_cat_df)\n",
        "police_onehot_array = one_hot.transform(police_cat_df).toarray()\n",
        "police_onehot_array = np.transpose(police_onehot_array)\n",
        "\n",
        "# numerical features\n",
        "numerical = [\"EventDate\", \"SubjectAge\", \"SubjectHeight\", \"SubjectWeight\"]\n",
        "police_norm_array = normalize(police_df[numerical], axis=0)\n",
        "police_norm_array = np.transpose(police_norm_array)\n",
        "\n",
        "# combine categorical and numerical\n",
        "police_transform_array = np.vstack([police_onehot_array, police_norm_array])\n",
        "police_transform_array = np.transpose(police_transform_array)\n",
        "\n",
        "# split the data into train and test\n",
        "x_train, x_test, y_train, y_test = train_test_split(police_transform_array, police_label_array, test_size=0.2, random_state=1234)\n",
        "\n",
        "# train model\n",
        "logistic_regression = LogisticRegression(random_state=0).fit(x_train, y_train)\n",
        "\n",
        "# accuracy on test set\n",
        "print(logistic_regression.score(x_train, y_train))\n",
        "print(logistic_regression.score(x_test, y_test))\n",
        "\n",
        "# predict classes\n",
        "logistic_regression.predict(x_test[:2, :])\n",
        "\n",
        "# predict probability\n",
        "logistic_regression.predict_proba(x_test[:2, :])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['EventDate', 'District', 'Zone', 'StopDescription', 'VehicleYear',\n",
            "       'VehicleMake', 'VehicleModel', 'VehicleStyle', 'VehicleColor',\n",
            "       'SubjectRace', 'SubjectGender', 'SubjectAge', 'SubjectHeight',\n",
            "       'SubjectWeight', 'SubjectHairColor', 'SubjectDriverLicState', 'Zip'],\n",
            "      dtype='object')\n",
            "0.89\n",
            "0.83\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.98459014, 0.01540986],\n",
              "       [0.84390874, 0.15609126]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dRTtRQjPEHYO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "cd4fe26b-a453-408f-9581-cb11d0544373"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "print(y.shape)\n",
        "print(type(y))"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(150,)\n",
            "<class 'numpy.ndarray'>\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}