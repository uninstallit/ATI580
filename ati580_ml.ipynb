{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ati580_ml.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyO03w1HEibL0Pwo8/ibHXR0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uninstallit/ati580_final_project/blob/edvin-1/ati580_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRsRJpBpCyr3",
        "colab_type": "text"
      },
      "source": [
        "**Machine Learning portion of ATI580 Final Project**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXeaEC9UDlom",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install dnspython"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iFcX8e-rMux",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "pip install --upgrade tf-nightly"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBeORpNUEj3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pymongo import MongoClient\n",
        "\n",
        "class Connect(object):\n",
        "\n",
        "  @staticmethod    \n",
        "  def get_connection(database):\n",
        "    username = \"mdbUser\"\n",
        "    password = \"ati580\"\n",
        "    return MongoClient('mongodb+srv://{}:{}@ati580-cluster.s5t5z.gcp.mongodb.net/{}?retryWrites=true&w=majority'.format(username, password, database))"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPXcZjGOa-id",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # save\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "# pip freeze --local > /content/gdrive/My\\ Drive/colab_installed.txt\n",
        "\n",
        "# # restore\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "# pip install --upgrade --force-reinstall `cat/content/gdrive/My\\ Drive/colab_installed.txt`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnUnpj-YbfmI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#plotly: https://plotly.com/python/ipython-notebook-tutorial/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b1PSfiEExeN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pymongo\n",
        "import datetime\n",
        "\n",
        "class PoliceInterviews(object):\n",
        "\n",
        "  def __init__(self, num_rows, output, eval_percent, batch):\n",
        "    # select columns\n",
        "    self._columns = dict({\n",
        "        'FieldInterviewID':0,     \n",
        "        'NOPD_Item':0,            \n",
        "        'EventDate':1,            \n",
        "        'District':1,             \n",
        "        'Zone':1,                 \n",
        "        'OfficerAssignment':0, \n",
        "        'StopDescription':1, \n",
        "        'ActionsTaken':0, \n",
        "        'VehicleYear':1, \n",
        "        'VehicleMake':1, \n",
        "        'VehicleModel':1, \n",
        "        'VehicleStyle':1, \n",
        "        'VehicleColor':1, \n",
        "        'SubjectID':0, \n",
        "        'SubjectRace':1, \n",
        "        'SubjectGender':1, \n",
        "        'SubjectAge':1, \n",
        "        'SubjectHasPhotoID':0, \n",
        "        'SubjectHeight':1, \n",
        "        'SubjectWeight':1, \n",
        "        'SubjectEyeColor':0, \n",
        "        'SubjectHairColor':1, \n",
        "        'SubjectDriverLicState':1, \n",
        "        'CreatedDateTime':0, \n",
        "        'LastModifiedDateTime':0, \n",
        "        'Longitude':0, \n",
        "        'Latitude':0, \n",
        "        'Zip':1, \n",
        "        'BlockAddress':0})\n",
        "\n",
        "    # connect to database\n",
        "    self._connect = Connect()\n",
        "    self._mdb_client = self._connect.get_connection(\"POLICE_DATABASE\")\n",
        "    self._mdb_database = self._mdb_client['POLICE_DATABASE'] \n",
        "    self._mdb_collection = self._mdb_database['POLICE_INTERVIEWS']\n",
        "    self._rows = num_rows\n",
        "\n",
        "    # data model attributes\n",
        "    self._output = output\n",
        "    self._eval_percent = eval_percent\n",
        "    self._batch = batch\n",
        "\n",
        "    # split the data into a training and validation set\n",
        "    # self._eval_df  = self._dataframe.sample(frac=self._eval_percent, random_state=123)\n",
        "    # self._train_df = self._dataframe.drop(self._eval_df.index)\n",
        "\n",
        "  # convert queries to dataframe or numpy array\n",
        "  def query_and_convert(self, filter, projection, to=\"dataframe\", rows=None):\n",
        "    if rows is not None:\n",
        "      _cursor = self._mdb_collection.find(filter, projection).limit(rows)\n",
        "    else:\n",
        "      _cursor = self._mdb_collection.find(filter, projection)\n",
        "    _dataframe = pd.DataFrame(list(_cursor))\n",
        "    if to is \"dataframe\":\n",
        "      return _dataframe\n",
        "    elif to is \"numpy\":\n",
        "      _np_array = np.transpose(np.squeeze(_dataframe.to_numpy()))\n",
        "      return _np_array\n",
        "  \n",
        "  # convert time to seconds \n",
        "  @staticmethod\n",
        "  def time_to_int_seconds(datetime_str):\n",
        "    time = datetime.datetime.strptime(datetime_str, '%m/%d/%Y %H:%M:%S %p').time()\n",
        "    [hours, minutes, seconds] = [int(t) for t in str(time).split(':')]\n",
        "    return datetime.timedelta(hours=hours, minutes=minutes, seconds=seconds).seconds\n",
        "\n",
        "  # convert license tag to binary\n",
        "  @staticmethod\n",
        "  def instate_or_outofstate(license):\n",
        "    if license is \"LA\":\n",
        "      return 1\n",
        "    return 0\n",
        "\n",
        "  # convert stop description to binary\n",
        "  @staticmethod\n",
        "  def cause_or_suspicion(description):\n",
        "    _probable_cause_list = [\"CRIMINAL VIOLATION\", \"JUVENILE VIOLATION\", \"TRAFFIC VIOLATION\"]\n",
        "    if description in _probable_cause_list:\n",
        "      return 0\n",
        "    return 1\n",
        "\n",
        "  def load_dataframe(self):\n",
        "    _filter = []\n",
        "    _projection = dict({'_id':0})\n",
        "    # create filter\n",
        "    for key, value in self._columns.items():\n",
        "      if value is 1:\n",
        "        _filter.append({key:{\"$exists\": True}})\n",
        "        _filter.append({key:{\"$ne\": \"\"}})\n",
        "        _filter.append({key:{\"$ne\": None}})\n",
        "        _filter.append({key:{\"$ne\": 0}})\n",
        "    # create projection\n",
        "    for key, value in self._columns.items():\n",
        "      if value is 1:\n",
        "        _projection[key] = value\n",
        "    # query database\n",
        "    _query_dataframe = self.query_and_convert({\"$and\":_filter}, _projection, rows=self._rows)\n",
        "    # convert specific columns\n",
        "    _query_dataframe['EventDate'] = _query_dataframe['EventDate'].apply(lambda x: self.time_to_int_seconds(x))\n",
        "    _query_dataframe['SubjectDriverLicState'] = _query_dataframe['SubjectDriverLicState'].apply(lambda x: self.instate_or_outofstate(x))\n",
        "    _query_dataframe['StopDescription'] = _query_dataframe['StopDescription'].apply(lambda x: self.cause_or_suspicion(x))\n",
        "    _query_dataframe['Zip'] = _query_dataframe['Zip'].apply(lambda x: int(x))\n",
        "    _query_dataframe['VehicleYear'] = _query_dataframe['VehicleYear'].apply(lambda x: int(x))\n",
        "    _query_dataframe['SubjectAge'] = _query_dataframe['SubjectAge'].apply(lambda x: int(x))\n",
        "    _query_dataframe['SubjectHeight'] = _query_dataframe['SubjectHeight'].apply(lambda x: int(x))\n",
        "    _query_dataframe['SubjectWeight'] = _query_dataframe['SubjectWeight'].apply(lambda x: int(x))\n",
        "\n",
        "    # split data into train and evaluate set\n",
        "    _eval_df = _query_dataframe.sample(frac=self._eval_percent, random_state=1234)\n",
        "    _train_df = _query_dataframe.drop(_eval_df.index)\n",
        "    return _train_df, _eval_df\n",
        "\n",
        "  # convert dataframe_to_dataset \n",
        "  @staticmethod\n",
        "  def dataframe_to_dataset(dataframe, output):\n",
        "    _dataframe = dataframe.copy()\n",
        "    _labels    = _dataframe.pop(output)\n",
        "    _dataset   = tf.data.Dataset.from_tensor_slices((dict(_dataframe), _labels))\n",
        "    _dataset   = _dataset.shuffle(buffer_size=len(_dataframe))\n",
        "    return _dataset\n",
        "\n",
        "  def load_dataset(self):\n",
        "    _train_df, _eval_df = self.load_dataframe()\n",
        "\n",
        "    _train_ds = self.dataframe_to_dataset(_train_df, self._output)\n",
        "    _eval_ds  = self.dataframe_to_dataset(_eval_df, self._output)\n",
        "\n",
        "    _train_ds = _train_ds.batch(self._batch)\n",
        "    _eval_ds  = _eval_ds.batch(self._batch)\n",
        "    return _train_ds, _eval_ds\n"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR3j6WP06CF1",
        "colab_type": "text"
      },
      "source": [
        "## **Classification with Neural Networks**\n",
        "- Source: [Keras | Code examples](https://keras.io/examples/structured_data/structured_data_classification_from_scratch/)\n",
        "- CategoricalEncoding API: [Tensorflow](https://tensorflow.google.cn/api_docs/python/tf/keras/layers/experimental/preprocessing/CategoryEncoding#methods)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjRg39ibH7yp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# pull data from db and convert\n",
        "police_interviews = PoliceInterviews(num_rows=1000, output=\"StopDescription\", eval_percent=0.2, batch=25)\n",
        "train_ds, eval_ds = police_interviews.load_dataset()\n",
        "\n",
        "# visual check\n",
        "# train_df, eval_df = police_interviews.load_dataframe()\n",
        "# print('{:<25} {} \\n'.format(\"Column Name\", \"Sample\"))\n",
        "# for (column_name, column_data) in train_df.iteritems():\n",
        "#   print('{:<25} {}'.format(column_name, column_data[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC92GRZJmEUd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
        "from tensorflow.keras.layers.experimental.preprocessing import CategoryEncoding\n",
        "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "\n",
        "def encode_numerical_feature(feature, name, dataset):\n",
        "    # Create a Normalization layer for our feature\n",
        "    normalizer = Normalization()\n",
        "    # Prepare a Dataset that only yields our feature\n",
        "    feature_ds = dataset.map(lambda x, y: x[name])\n",
        "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
        "    # Learn the statistics of the data\n",
        "    normalizer.adapt(feature_ds)\n",
        "    # Normalize the input feature\n",
        "    encoded_feature = normalizer(feature)\n",
        "    return encoded_feature\n",
        "\n",
        "def encode_string_categorical_feature(feature, name, dataset):\n",
        "    # Create a StringLookup layer which will turn strings into integer indices\n",
        "    index = StringLookup()\n",
        "    # Prepare a Dataset that only yields our feature\n",
        "    feature_ds = dataset.map(lambda x, y: x[name])\n",
        "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
        "    # Learn the set of possible string values and assign them a fixed integer index\n",
        "    index.adapt(feature_ds)\n",
        "    # Turn the string input into integer indices\n",
        "    encoded_feature = index(feature)\n",
        "    # Create a CategoryEncoding for our integer indices\n",
        "    encoder = CategoryEncoding(output_mode=\"binary\")\n",
        "    # Prepare a dataset of indices\n",
        "    feature_ds = feature_ds.map(index)\n",
        "    # Learn the space of possible indices\n",
        "    encoder.adapt(feature_ds)\n",
        "    # Apply one-hot encoding to our indices\n",
        "    encoded_feature = encoder(encoded_feature)\n",
        "    return encoded_feature\n",
        "\n",
        "def encode_integer_categorical_feature(feature, name, dataset):\n",
        "    # Create a CategoryEncoding for our integer indices\n",
        "    encoder = CategoryEncoding(output_mode=\"binary\")\n",
        "    # Prepare a Dataset that only yields our feature\n",
        "    feature_ds = dataset.map(lambda x, y: x[name])\n",
        "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
        "    # Learn the space of possible indices\n",
        "    encoder.adapt(feature_ds)\n",
        "    # Apply one-hot encoding to our indices\n",
        "    encoded_feature = encoder(feature)\n",
        "    return encoded_feature\n",
        "\n",
        "# categorical features encoded as integers\n",
        "inout_state  = keras.Input(shape=(1,), name=\"SubjectDriverLicState\", dtype=\"int64\")\n",
        "district     = keras.Input(shape=(1,), name=\"District\", dtype=\"int64\")\n",
        "vehicle_year = keras.Input(shape=(1,), name=\"VehicleYear\", dtype=\"int64\")\n",
        "zip          = keras.Input(shape=(1,), name=\"Zip\", dtype=\"int64\")\n",
        "# output\n",
        "# stop_description = keras.Input(shape=(1,), name=\"StopDescription\", dtype=\"int64\")\n",
        "\n",
        "# categorical feature encoded as string\n",
        "zone             = keras.Input(shape=(1,), name=\"Zone\", dtype=\"string\")\n",
        "vehicle_make     = keras.Input(shape=(1,), name=\"VehicleMake\", dtype=\"string\")\n",
        "vehicle_model    = keras.Input(shape=(1,), name=\"VehicleModel\", dtype=\"string\")\n",
        "vehicle_style    = keras.Input(shape=(1,), name=\"VehicleStyle\", dtype=\"string\")\n",
        "vehicle_color    = keras.Input(shape=(1,), name=\"VehicleColor\", dtype=\"string\")\n",
        "subject_race     = keras.Input(shape=(1,), name=\"SubjectRace\", dtype=\"string\")\n",
        "subject_gender   = keras.Input(shape=(1,), name=\"SubjectGender\", dtype=\"string\")\n",
        "hair_color       = keras.Input(shape=(1,), name=\"SubjectHairColor\", dtype=\"string\")\n",
        "\n",
        "# numerical features\n",
        "time           = keras.Input(shape=(1,), name=\"EventDate\")\n",
        "subject_age    = keras.Input(shape=(1,), name=\"SubjectAge\")\n",
        "subject_height = keras.Input(shape=(1,), name=\"SubjectHeight\")\n",
        "subject_weight = keras.Input(shape=(1,), name=\"SubjectWeight\")\n",
        "\n",
        "all_inputs = [inout_state,\n",
        "              district, \n",
        "              vehicle_year, \n",
        "              zip, \n",
        "              zone, \n",
        "              vehicle_make, \n",
        "              vehicle_model, \n",
        "              vehicle_style, \n",
        "              vehicle_color, \n",
        "              subject_race, \n",
        "              subject_gender, \n",
        "              hair_color, \n",
        "              time, \n",
        "              subject_age, \n",
        "              subject_height, \n",
        "              subject_weight]\n",
        "\n",
        "# integer categorical features\n",
        "inout_state_encoded  = encode_integer_categorical_feature(inout_state, \"SubjectDriverLicState\", train_ds)\n",
        "district_encoded     = encode_integer_categorical_feature(district, \"District\", train_ds)\n",
        "vehicle_year_encoded = encode_integer_categorical_feature(vehicle_year, \"VehicleYear\", train_ds)\n",
        "zip_encoded          = encode_integer_categorical_feature(zip, \"Zip\", train_ds)\n",
        "\n",
        "# string categorical features\n",
        "zone_encoded             = encode_string_categorical_feature(zone, \"Zone\", train_ds)\n",
        "vehicle_make_encoded     = encode_string_categorical_feature(vehicle_make, \"VehicleMake\", train_ds)\n",
        "vehicle_model_encoded    = encode_string_categorical_feature(vehicle_model, \"VehicleModel\", train_ds)\n",
        "vehicle_style_encoded    = encode_string_categorical_feature(vehicle_style, \"VehicleStyle\", train_ds)\n",
        "vehicle_color_encoded    = encode_string_categorical_feature(vehicle_color, \"VehicleColor\", train_ds)\n",
        "subject_race_encoded     = encode_string_categorical_feature(subject_race, \"SubjectRace\", train_ds)\n",
        "subject_gender_encoded   = encode_string_categorical_feature(subject_gender, \"SubjectGender\", train_ds)\n",
        "hair_color_encoded       = encode_string_categorical_feature(hair_color, \"SubjectHairColor\", train_ds)\n",
        "\n",
        "# numerical features\n",
        "time_encoded           = encode_numerical_feature(time, \"EventDate\", train_ds)\n",
        "subject_age_encoded    = encode_numerical_feature(subject_age, \"SubjectAge\", train_ds)\n",
        "subject_height_encoded = encode_numerical_feature(subject_height, \"SubjectHeight\", train_ds)\n",
        "subject_weight_encoded = encode_numerical_feature(subject_weight, \"SubjectWeight\", train_ds)\n",
        "\n",
        "all_features = layers.concatenate([\n",
        "                                   inout_state_encoded,\n",
        "                                   district_encoded,\n",
        "                                   vehicle_year_encoded,\n",
        "                                   zip_encoded,\n",
        "                                   zone_encoded,\n",
        "                                   vehicle_make_encoded,\n",
        "                                   vehicle_model_encoded,\n",
        "                                   vehicle_style_encoded,\n",
        "                                   vehicle_color_encoded,\n",
        "                                   subject_race_encoded,\n",
        "                                   subject_gender_encoded,\n",
        "                                   hair_color_encoded,\n",
        "                                   time_encoded,\n",
        "                                   subject_age_encoded,\n",
        "                                   subject_height_encoded,\n",
        "                                   subject_weight_encoded])\n",
        "\n",
        "# build model\n",
        "x = layers.Dense(32, activation=\"relu\")(all_features)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "x = layers.Dense(16, activation=\"sigmoid\")(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "x = layers.Dense(8, activation=\"sigmoid\")(x)\n",
        "output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(all_inputs, output)\n",
        "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F9BjVItnr9Gy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# model diagram\n",
        "keras.utils.plot_model(model, show_shapes=True, rankdir=\"LR\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_geQsn5ssog",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train model\n",
        "model.fit(train_ds, epochs=5, validation_data=eval_ds)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxGLH6CmHTC2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# predict from model\n",
        "sample = dict({\n",
        "    \"SubjectDriverLicState\":0,\n",
        "    \"District\":7,\n",
        "    \"VehicleYear\":1997,\n",
        "    \"Zip\":70127,\n",
        "    \"Zone\":\"O\",\n",
        "    \"VehicleMake\":\"CHEVROLET\",\n",
        "    \"VehicleModel\":\"TAHOE\",\n",
        "    \"VehicleStyle\":\"SPORTS UTILITY\",\n",
        "    \"VehicleColor\":\"BLUE\",\n",
        "    \"SubjectRace\":\"BLACK\",\n",
        "    \"SubjectGender\":\"MALE\",\n",
        "    \"SubjectHairColor\":\"Black\",\n",
        "    \"EventDate\":34500,\n",
        "    \"SubjectAge\":21,\n",
        "    \"SubjectHeight\":72,\n",
        "    \"SubjectWeight\":169,\n",
        "})\n",
        "\n",
        "input_dict = {name: tf.convert_to_tensor([value]) for name, value in sample.items()}\n",
        "tf.print(input_dict)\n",
        "model.predict(input_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdB5Xh2Q9dc9",
        "colab_type": "text"
      },
      "source": [
        "## **Logistic Regression**\n",
        "\n",
        "Binary Logistic Regression, in which the target variable has only two possible values, e.g., pass/fail or win/lose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7N_QcET9cH6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "71f9c9c0-f40c-4cb9-d1ee-69441a4f7c23"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "x, y = load_iris(return_X_y=True)\n",
        "\n",
        "columns = dict({'FieldInterviewID':0,     \n",
        "                'NOPD_Item':0, \n",
        "                'EventDate':0, \n",
        "                'District':0, \n",
        "                'Zone':0, \n",
        "                'OfficerAssignment':0, \n",
        "                'StopDescription':0, \n",
        "                'ActionsTaken':0, \n",
        "                'VehicleYear':1, \n",
        "                'VehicleMake':0, \n",
        "                'VehicleModel':0, \n",
        "                'VehicleStyle':0, \n",
        "                'VehicleColor':1, \n",
        "                'SubjectID':0, \n",
        "                'SubjectRace':1, \n",
        "                'SubjectGender':1, \n",
        "                'SubjectAge':0, \n",
        "                'SubjectHasPhotoID':0, \n",
        "                'SubjectHeight':1, \n",
        "                'SubjectWeight':0, \n",
        "                'SubjectEyeColor':0, \n",
        "                'SubjectHairColor':1, \n",
        "                'SubjectDriverLicState':0, \n",
        "                'CreatedDateTime':0, \n",
        "                'LastModifiedDateTime':0, \n",
        "                'Longitude':0, \n",
        "                'Latitude':0, \n",
        "                'Zip':1, \n",
        "                'BlockAddress':0})\n",
        "\n",
        "print(x[:100])\n",
        "print(y[:100])\n",
        "\n",
        "clf = LogisticRegression(random_state=0).fit(X, y)\n",
        "\n",
        "clf.predict(X[:2, :])\n",
        "array([0, 0])\n",
        "\n",
        "clf.predict_proba(X[:2, :])\n",
        "array([[9.8...e-01, 1.8...e-02, 1.4...e-08],\n",
        "       [9.7...e-01, 2.8...e-02, ...e-08]])\n",
        "clf.score(X, y)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5.1 3.5 1.4 0.2]\n",
            " [4.9 3.  1.4 0.2]\n",
            " [4.7 3.2 1.3 0.2]\n",
            " [4.6 3.1 1.5 0.2]\n",
            " [5.  3.6 1.4 0.2]\n",
            " [5.4 3.9 1.7 0.4]\n",
            " [4.6 3.4 1.4 0.3]\n",
            " [5.  3.4 1.5 0.2]\n",
            " [4.4 2.9 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.1]\n",
            " [5.4 3.7 1.5 0.2]\n",
            " [4.8 3.4 1.6 0.2]\n",
            " [4.8 3.  1.4 0.1]\n",
            " [4.3 3.  1.1 0.1]\n",
            " [5.8 4.  1.2 0.2]\n",
            " [5.7 4.4 1.5 0.4]\n",
            " [5.4 3.9 1.3 0.4]\n",
            " [5.1 3.5 1.4 0.3]\n",
            " [5.7 3.8 1.7 0.3]\n",
            " [5.1 3.8 1.5 0.3]\n",
            " [5.4 3.4 1.7 0.2]\n",
            " [5.1 3.7 1.5 0.4]\n",
            " [4.6 3.6 1.  0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [4.8 3.4 1.9 0.2]\n",
            " [5.  3.  1.6 0.2]\n",
            " [5.  3.4 1.6 0.4]\n",
            " [5.2 3.5 1.5 0.2]\n",
            " [5.2 3.4 1.4 0.2]\n",
            " [4.7 3.2 1.6 0.2]\n",
            " [4.8 3.1 1.6 0.2]\n",
            " [5.4 3.4 1.5 0.4]\n",
            " [5.2 4.1 1.5 0.1]\n",
            " [5.5 4.2 1.4 0.2]\n",
            " [4.9 3.1 1.5 0.2]\n",
            " [5.  3.2 1.2 0.2]\n",
            " [5.5 3.5 1.3 0.2]\n",
            " [4.9 3.6 1.4 0.1]\n",
            " [4.4 3.  1.3 0.2]\n",
            " [5.1 3.4 1.5 0.2]\n",
            " [5.  3.5 1.3 0.3]\n",
            " [4.5 2.3 1.3 0.3]\n",
            " [4.4 3.2 1.3 0.2]\n",
            " [5.  3.5 1.6 0.6]\n",
            " [5.1 3.8 1.9 0.4]\n",
            " [4.8 3.  1.4 0.3]\n",
            " [5.1 3.8 1.6 0.2]\n",
            " [4.6 3.2 1.4 0.2]\n",
            " [5.3 3.7 1.5 0.2]\n",
            " [5.  3.3 1.4 0.2]\n",
            " [7.  3.2 4.7 1.4]\n",
            " [6.4 3.2 4.5 1.5]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [5.5 2.3 4.  1.3]\n",
            " [6.5 2.8 4.6 1.5]\n",
            " [5.7 2.8 4.5 1.3]\n",
            " [6.3 3.3 4.7 1.6]\n",
            " [4.9 2.4 3.3 1. ]\n",
            " [6.6 2.9 4.6 1.3]\n",
            " [5.2 2.7 3.9 1.4]\n",
            " [5.  2.  3.5 1. ]\n",
            " [5.9 3.  4.2 1.5]\n",
            " [6.  2.2 4.  1. ]\n",
            " [6.1 2.9 4.7 1.4]\n",
            " [5.6 2.9 3.6 1.3]\n",
            " [6.7 3.1 4.4 1.4]\n",
            " [5.6 3.  4.5 1.5]\n",
            " [5.8 2.7 4.1 1. ]\n",
            " [6.2 2.2 4.5 1.5]\n",
            " [5.6 2.5 3.9 1.1]\n",
            " [5.9 3.2 4.8 1.8]\n",
            " [6.1 2.8 4.  1.3]\n",
            " [6.3 2.5 4.9 1.5]\n",
            " [6.1 2.8 4.7 1.2]\n",
            " [6.4 2.9 4.3 1.3]\n",
            " [6.6 3.  4.4 1.4]\n",
            " [6.8 2.8 4.8 1.4]\n",
            " [6.7 3.  5.  1.7]\n",
            " [6.  2.9 4.5 1.5]\n",
            " [5.7 2.6 3.5 1. ]\n",
            " [5.5 2.4 3.8 1.1]\n",
            " [5.5 2.4 3.7 1. ]\n",
            " [5.8 2.7 3.9 1.2]\n",
            " [6.  2.7 5.1 1.6]\n",
            " [5.4 3.  4.5 1.5]\n",
            " [6.  3.4 4.5 1.6]\n",
            " [6.7 3.1 4.7 1.5]\n",
            " [6.3 2.3 4.4 1.3]\n",
            " [5.6 3.  4.1 1.3]\n",
            " [5.5 2.5 4.  1.3]\n",
            " [5.5 2.6 4.4 1.2]\n",
            " [6.1 3.  4.6 1.4]\n",
            " [5.8 2.6 4.  1.2]\n",
            " [5.  2.3 3.3 1. ]\n",
            " [5.6 2.7 4.2 1.3]\n",
            " [5.7 3.  4.2 1.2]\n",
            " [5.7 2.9 4.2 1.3]\n",
            " [6.2 2.9 4.3 1.3]\n",
            " [5.1 2.5 3.  1.1]\n",
            " [5.7 2.8 4.1 1.3]]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}