{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ati580_ml.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPugh/6jCOpeFrA04UJKCaE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uninstallit/ati580_final_project/blob/edvin-1/ati580_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRsRJpBpCyr3",
        "colab_type": "text"
      },
      "source": [
        "**Machine Learning portion of ATI580 Final Project**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXeaEC9UDlom",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "1b17038e-3973-44b2-de96-126264b8cd44"
      },
      "source": [
        "pip install dnspython"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting dnspython\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ec/d3/3aa0e7213ef72b8585747aa0e271a9523e713813b9a20177ebe1e939deb0/dnspython-1.16.0-py2.py3-none-any.whl (188kB)\n",
            "\u001b[K     |████████████████████████████████| 194kB 2.8MB/s \n",
            "\u001b[?25hInstalling collected packages: dnspython\n",
            "Successfully installed dnspython-1.16.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iFcX8e-rMux",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 800
        },
        "outputId": "1b3ab376-e8a6-4330-bfe8-923f38f1ef89"
      },
      "source": [
        "pip install --upgrade tf-nightly"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/53/a6/ba3726c7e7952a19537260de9a069a504347b385cfa1af4e1e702cde956d/tf_nightly-2.4.0.dev20200714-cp36-cp36m-manylinux2010_x86_64.whl (142.9MB)\n",
            "\u001b[K     |████████████████████████████████| 142.9MB 90kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.2.0)\n",
            "Requirement already satisfied, skipping upgrade: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12.1)\n",
            "Requirement already satisfied, skipping upgrade: numpy<1.19.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.30.0)\n",
            "Requirement already satisfied, skipping upgrade: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.2)\n",
            "Requirement already satisfied, skipping upgrade: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (2.10.0)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.10.0)\n",
            "Collecting tb-nightly<2.4.0a0,>=2.3.0a0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/9f/3da1e2cd10cc100cb1ea1b304a329aadec3f809c0b861525b1a4093b0736/tb_nightly-2.3.0a20200713-py3-none-any.whl (6.8MB)\n",
            "\u001b[K     |████████████████████████████████| 6.8MB 49.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.1.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.3.3)\n",
            "Collecting tf-estimator-nightly\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/49/2e/478d32bb674716865a650a3c41fa2f84683b11bc2ca889859bd9950a5786/tf_estimator_nightly-2.4.0.dev2020071401-py2.py3-none-any.whl (459kB)\n",
            "\u001b[K     |████████████████████████████████| 460kB 46.8MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (3.2.1)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (0.34.2)\n",
            "Requirement already satisfied, skipping upgrade: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly) (1.6.3)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tf-nightly) (49.1.0)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.17.2)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (4.1.1)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (4.6)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.7.0)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tb-nightly<2.4.0a0,>=2.3.0a0->tf-nightly) (3.1.0)\n",
            "Installing collected packages: tb-nightly, tf-estimator-nightly, tf-nightly\n",
            "Successfully installed tb-nightly-2.3.0a20200713 tf-estimator-nightly-2.4.0.dev2020071401 tf-nightly-2.4.0.dev20200714\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBeORpNUEj3-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pymongo import MongoClient\n",
        "\n",
        "class Connect(object):\n",
        "\n",
        "  @staticmethod    \n",
        "  def get_connection(database):\n",
        "    username = \"mdbUser\"\n",
        "    password = \"ati580\"\n",
        "    return MongoClient('mongodb+srv://{}:{}@ati580-cluster.s5t5z.gcp.mongodb.net/{}?retryWrites=true&w=majority'.format(username, password, database))"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPXcZjGOa-id",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# # save\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "# pip freeze --local > /content/gdrive/My\\ Drive/colab_installed.txt\n",
        "\n",
        "# # restore\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive')\n",
        "# pip install --upgrade --force-reinstall `cat/content/gdrive/My\\ Drive/colab_installed.txt`"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnUnpj-YbfmI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#plotly: https://plotly.com/python/ipython-notebook-tutorial/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8b1PSfiEExeN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from IPython.display import clear_output\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import pymongo\n",
        "import datetime\n",
        "\n",
        "class PoliceInterviews(object):\n",
        "\n",
        "  def __init__(self, selected, num_rows, output, eval_percent, batch):\n",
        "    self._columns = selected\n",
        "    self._rows = num_rows\n",
        "    # data model attributes\n",
        "    self._output = output\n",
        "    self._eval_percent = eval_percent\n",
        "    self._batch = batch\n",
        "    self._query_dataframe = pd.DataFrame([])\n",
        "    # connect to database\n",
        "    self._connect = Connect()\n",
        "    self._mdb_client = self._connect.get_connection(\"POLICE_DATABASE\")\n",
        "    self._mdb_database = self._mdb_client['POLICE_DATABASE'] \n",
        "    self._mdb_collection = self._mdb_database['POLICE_INTERVIEWS']\n",
        "    self._is_retrieved = False \n",
        "\n",
        "  # convert queries to dataframe or numpy array\n",
        "  def query_and_convert(self, filter, projection, to=\"dataframe\", rows=None):\n",
        "    if rows is not None:\n",
        "      _cursor = self._mdb_collection.find(filter, projection).limit(rows)\n",
        "    else:\n",
        "      _cursor = self._mdb_collection.find(filter, projection)\n",
        "    _dataframe = pd.DataFrame(list(_cursor))\n",
        "    if to == \"dataframe\":\n",
        "      return _dataframe\n",
        "    elif to == \"numpy\":\n",
        "      _np_array = np.transpose(np.squeeze(_dataframe.to_numpy()))\n",
        "      return _np_array\n",
        "  \n",
        "  # convert time to seconds \n",
        "  @staticmethod\n",
        "  def time_to_int_seconds(datetime_str):\n",
        "    time = datetime.datetime.strptime(datetime_str, '%m/%d/%Y %H:%M:%S %p').time()\n",
        "    [hours, minutes, seconds] = [int(t) for t in str(time).split(':')]\n",
        "    return datetime.timedelta(hours=hours, minutes=minutes, seconds=seconds).seconds\n",
        "\n",
        "  # convert license tag to binary\n",
        "  @staticmethod\n",
        "  def instate_or_outofstate(license):\n",
        "    if license == \"LA\":\n",
        "      return 1\n",
        "    return 0\n",
        "\n",
        "  # convert stop description to binary\n",
        "  @staticmethod\n",
        "  def cause_or_suspicion(description):\n",
        "    _probable_cause_list = [\"CRIMINAL VIOLATION\", \"JUVENILE VIOLATION\", \"TRAFFIC VIOLATION\"]\n",
        "    if description in _probable_cause_list:\n",
        "      return 0\n",
        "    return 1\n",
        "\n",
        "  def query_database(self):\n",
        "    print(\"Pulling data ...\")\n",
        "    _filter = []\n",
        "    _projection = dict({'_id':0})\n",
        "    # create filter\n",
        "    for key, value in self._columns.items():\n",
        "      if value == 1:\n",
        "        _filter.append({key:{\"$exists\": True}})\n",
        "        _filter.append({key:{\"$ne\": \"\"}})\n",
        "        _filter.append({key:{\"$ne\": None}})\n",
        "        _filter.append({key:{\"$ne\": 0}})\n",
        "    # create projection\n",
        "    for key, value in self._columns.items():\n",
        "      if value == 1:\n",
        "        _projection[key] = value\n",
        "    # query database\n",
        "    self._query_dataframe = self.query_and_convert({\"$and\":_filter}, _projection, rows=self._rows)\n",
        "    clear_output()\n",
        "\n",
        "  def load_dataframe(self, split=True):\n",
        "    if self._is_retrieved is False:\n",
        "      self.query_database()\n",
        "      self._is_retrieved = True\n",
        "    # convert specific columns\n",
        "    _dataframe = self._query_dataframe.copy()\n",
        "    for key, value in self._columns.items():\n",
        "      if value == 1 and key == 'EventDate':\n",
        "        _dataframe['EventDate'] = self._query_dataframe['EventDate'].apply(lambda x: self.time_to_int_seconds(x))\n",
        "      elif value == 1 and key == 'SubjectDriverLicState':\n",
        "        _dataframe['SubjectDriverLicState'] = self._query_dataframe['SubjectDriverLicState'].apply(lambda x: self.instate_or_outofstate(x))\n",
        "      elif value == 1 and key == 'StopDescription':\n",
        "        _dataframe['StopDescription'] = self._query_dataframe['StopDescription'].apply(lambda x: self.cause_or_suspicion(x))\n",
        "      elif value == 1 and key == 'Zip':\n",
        "        _dataframe['Zip'] = self._query_dataframe['Zip'].apply(lambda x: int(x))\n",
        "      elif value == 1 and key == 'VehicleYear':\n",
        "        _dataframe['VehicleYear'] = self._query_dataframe['VehicleYear'].apply(lambda x: int(x))\n",
        "      elif value == 1 and key == 'SubjectAge':\n",
        "        _dataframe['SubjectAge'] = self._query_dataframe['SubjectAge'].apply(lambda x: int(x))\n",
        "      elif value == 1 and key == 'SubjectHeight':\n",
        "        _dataframe['SubjectHeight'] = self._query_dataframe['SubjectHeight'].apply(lambda x: int(x))\n",
        "      elif value == 1 and key == 'SubjectWeight':\n",
        "        _dataframe['SubjectWeight'] = self._query_dataframe['SubjectWeight'].apply(lambda x: int(x))\n",
        "    # split data into train and evaluate set\n",
        "    if split is True:\n",
        "      _eval_df = _dataframe.sample(frac=self._eval_percent, random_state=1234)\n",
        "      _train_df = _dataframe.drop(_eval_df.index)\n",
        "      return _train_df, _eval_df\n",
        "    return _dataframe\n",
        "\n",
        "  # convert dataframe_to_dataset \n",
        "  @staticmethod\n",
        "  def dataframe_to_dataset(dataframe, output):\n",
        "    _dataframe = dataframe.copy()\n",
        "    _labels    = _dataframe.pop(output)\n",
        "    _dataset   = tf.data.Dataset.from_tensor_slices((dict(_dataframe), _labels))\n",
        "    _dataset   = _dataset.shuffle(buffer_size=len(_dataframe))\n",
        "    return _dataset\n",
        "\n",
        "  def load_dataset(self):\n",
        "    _train_df, _eval_df = self.load_dataframe()\n",
        "    _train_ds = self.dataframe_to_dataset(_train_df, self._output)\n",
        "    _eval_ds  = self.dataframe_to_dataset(_eval_df, self._output)\n",
        "    # batch dataset\n",
        "    _train_ds = _train_ds.batch(self._batch)\n",
        "    _eval_ds  = _eval_ds.batch(self._batch)\n",
        "    return _train_ds, _eval_ds\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR3j6WP06CF1",
        "colab_type": "text"
      },
      "source": [
        "## **Classification with Neural Networks**\n",
        "- Source: [Keras | Code examples](https://keras.io/examples/structured_data/structured_data_classification_from_scratch/)\n",
        "- CategoricalEncoding API: [Tensorflow](https://tensorflow.google.cn/api_docs/python/tf/keras/layers/experimental/preprocessing/CategoryEncoding#methods)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjRg39ibH7yp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# select columns\n",
        "columns = dict({\n",
        "    'FieldInterviewID':0,     \n",
        "    'NOPD_Item':0,            \n",
        "    'EventDate':1,            \n",
        "    'District':1,             \n",
        "    'Zone':1,                 \n",
        "    'OfficerAssignment':0, \n",
        "    'StopDescription':1, \n",
        "    'ActionsTaken':0, \n",
        "    'VehicleYear':1, \n",
        "    'VehicleMake':1, \n",
        "    'VehicleModel':1, \n",
        "    'VehicleStyle':1, \n",
        "    'VehicleColor':1, \n",
        "    'SubjectID':0, \n",
        "    'SubjectRace':1, \n",
        "    'SubjectGender':1, \n",
        "    'SubjectAge':1, \n",
        "    'SubjectHasPhotoID':0, \n",
        "    'SubjectHeight':1, \n",
        "    'SubjectWeight':1, \n",
        "    'SubjectEyeColor':0, \n",
        "    'SubjectHairColor':1, \n",
        "    'SubjectDriverLicState':1, \n",
        "    'CreatedDateTime':0, \n",
        "    'LastModifiedDateTime':0, \n",
        "    'Longitude':1, \n",
        "    'Latitude':1, \n",
        "    'Zip':1, \n",
        "    'BlockAddress':0})\n",
        "\n",
        "# pull data from db and convert\n",
        "police_interviews = PoliceInterviews(columns, num_rows=None, output=\"StopDescription\", eval_percent=0.2, batch=25)\n",
        "train_ds, eval_ds = police_interviews.load_dataset()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okbFnEeMJXM0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "outputId": "28248d93-0fd0-4140-d946-82f2db21406a"
      },
      "source": [
        "# visual check\n",
        "train_df, eval_df = police_interviews.load_dataframe()\n",
        "print('{:<25} {} \\n'.format(\"Column Name\", \"Sample\"))\n",
        "for (column_name, column_data) in train_df.iteritems():\n",
        "  print('{:<25} {}'.format(column_name, column_data[0]))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Column Name               Sample \n",
            "\n",
            "EventDate                 34500\n",
            "District                  7\n",
            "Zone                      O\n",
            "StopDescription           0\n",
            "VehicleYear               1997\n",
            "VehicleMake               CHEVROLET\n",
            "VehicleModel              TAHOE\n",
            "VehicleStyle              SPORTS UTILITY\n",
            "VehicleColor              BLUE\n",
            "SubjectRace               BLACK\n",
            "SubjectGender             MALE\n",
            "SubjectAge                21\n",
            "SubjectHeight             72\n",
            "SubjectWeight             169\n",
            "SubjectHairColor          Black\n",
            "SubjectDriverLicState     1\n",
            "Longitude                 -89.9887508371\n",
            "Latitude                  30.0355783772\n",
            "Zip                       70127\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC92GRZJmEUd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
        "from tensorflow.keras.layers.experimental.preprocessing import CategoryEncoding\n",
        "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow import keras\n",
        "\n",
        "def encode_numerical_feature(feature, name, dataset):\n",
        "    # Create a Normalization layer for our feature\n",
        "    normalizer = Normalization()\n",
        "    # Prepare a Dataset that only yields our feature\n",
        "    feature_ds = dataset.map(lambda x, y: x[name])\n",
        "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
        "    # Learn the statistics of the data\n",
        "    normalizer.adapt(feature_ds)\n",
        "    # Normalize the input feature\n",
        "    encoded_feature = normalizer(feature)\n",
        "    return encoded_feature\n",
        "\n",
        "def encode_string_categorical_feature(feature, name, dataset):\n",
        "    # Create a StringLookup layer which will turn strings into integer indices\n",
        "    index = StringLookup()\n",
        "    # Prepare a Dataset that only yields our feature\n",
        "    feature_ds = dataset.map(lambda x, y: x[name])\n",
        "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
        "    # Learn the set of possible string values and assign them a fixed integer index\n",
        "    index.adapt(feature_ds)\n",
        "    # Turn the string input into integer indices\n",
        "    encoded_feature = index(feature)\n",
        "    # Create a CategoryEncoding for our integer indices\n",
        "    encoder = CategoryEncoding(output_mode=\"binary\")\n",
        "    # Prepare a dataset of indices\n",
        "    feature_ds = feature_ds.map(index)\n",
        "    # Learn the space of possible indices\n",
        "    encoder.adapt(feature_ds)\n",
        "    # Apply one-hot encoding to our indices\n",
        "    encoded_feature = encoder(encoded_feature)\n",
        "    return encoded_feature\n",
        "\n",
        "def encode_integer_categorical_feature(feature, name, dataset):\n",
        "    # Create a CategoryEncoding for our integer indices\n",
        "    encoder = CategoryEncoding(output_mode=\"binary\")\n",
        "    # Prepare a Dataset that only yields our feature\n",
        "    feature_ds = dataset.map(lambda x, y: x[name])\n",
        "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
        "    # Learn the space of possible indices\n",
        "    encoder.adapt(feature_ds)\n",
        "    # Apply one-hot encoding to our indices\n",
        "    encoded_feature = encoder(feature)\n",
        "    return encoded_feature\n",
        "\n",
        "# categorical features encoded as integers\n",
        "inout_state  = keras.Input(shape=(1,), name=\"SubjectDriverLicState\", dtype=\"int64\")\n",
        "district     = keras.Input(shape=(1,), name=\"District\", dtype=\"int64\")\n",
        "vehicle_year = keras.Input(shape=(1,), name=\"VehicleYear\", dtype=\"int64\")\n",
        "zip          = keras.Input(shape=(1,), name=\"Zip\", dtype=\"int64\")\n",
        "# output\n",
        "# stop_description = keras.Input(shape=(1,), name=\"StopDescription\", dtype=\"int64\")\n",
        "\n",
        "# categorical feature encoded as string\n",
        "zone             = keras.Input(shape=(1,), name=\"Zone\", dtype=\"string\")\n",
        "vehicle_make     = keras.Input(shape=(1,), name=\"VehicleMake\", dtype=\"string\")\n",
        "vehicle_model    = keras.Input(shape=(1,), name=\"VehicleModel\", dtype=\"string\")\n",
        "vehicle_style    = keras.Input(shape=(1,), name=\"VehicleStyle\", dtype=\"string\")\n",
        "vehicle_color    = keras.Input(shape=(1,), name=\"VehicleColor\", dtype=\"string\")\n",
        "subject_race     = keras.Input(shape=(1,), name=\"SubjectRace\", dtype=\"string\")\n",
        "subject_gender   = keras.Input(shape=(1,), name=\"SubjectGender\", dtype=\"string\")\n",
        "hair_color       = keras.Input(shape=(1,), name=\"SubjectHairColor\", dtype=\"string\")\n",
        "\n",
        "# numerical features\n",
        "time           = keras.Input(shape=(1,), name=\"EventDate\")\n",
        "subject_age    = keras.Input(shape=(1,), name=\"SubjectAge\")\n",
        "subject_height = keras.Input(shape=(1,), name=\"SubjectHeight\")\n",
        "subject_weight = keras.Input(shape=(1,), name=\"SubjectWeight\")\n",
        "longitude      = keras.Input(shape=(1,), name=\"Longitude\")\n",
        "latitude       = keras.Input(shape=(1,), name=\"Latitude\")\n",
        "\n",
        "all_inputs = [inout_state,\n",
        "              district, \n",
        "              vehicle_year, \n",
        "              zip, \n",
        "              zone, \n",
        "              vehicle_make, \n",
        "              vehicle_model, \n",
        "              vehicle_style, \n",
        "              vehicle_color, \n",
        "              subject_race, \n",
        "              subject_gender, \n",
        "              hair_color, \n",
        "              time, \n",
        "              subject_age, \n",
        "              subject_height, \n",
        "              subject_weight,\n",
        "              longitude,\n",
        "              latitude]\n",
        "\n",
        "# integer categorical features\n",
        "inout_state_encoded  = encode_integer_categorical_feature(inout_state, \"SubjectDriverLicState\", train_ds)\n",
        "district_encoded     = encode_integer_categorical_feature(district, \"District\", train_ds)\n",
        "vehicle_year_encoded = encode_integer_categorical_feature(vehicle_year, \"VehicleYear\", train_ds)\n",
        "zip_encoded          = encode_integer_categorical_feature(zip, \"Zip\", train_ds)\n",
        "\n",
        "# string categorical features\n",
        "zone_encoded             = encode_string_categorical_feature(zone, \"Zone\", train_ds)\n",
        "vehicle_make_encoded     = encode_string_categorical_feature(vehicle_make, \"VehicleMake\", train_ds)\n",
        "vehicle_model_encoded    = encode_string_categorical_feature(vehicle_model, \"VehicleModel\", train_ds)\n",
        "vehicle_style_encoded    = encode_string_categorical_feature(vehicle_style, \"VehicleStyle\", train_ds)\n",
        "vehicle_color_encoded    = encode_string_categorical_feature(vehicle_color, \"VehicleColor\", train_ds)\n",
        "subject_race_encoded     = encode_string_categorical_feature(subject_race, \"SubjectRace\", train_ds)\n",
        "subject_gender_encoded   = encode_string_categorical_feature(subject_gender, \"SubjectGender\", train_ds)\n",
        "hair_color_encoded       = encode_string_categorical_feature(hair_color, \"SubjectHairColor\", train_ds)\n",
        "\n",
        "# numerical features\n",
        "time_encoded           = encode_numerical_feature(time, \"EventDate\", train_ds)\n",
        "subject_age_encoded    = encode_numerical_feature(subject_age, \"SubjectAge\", train_ds)\n",
        "subject_height_encoded = encode_numerical_feature(subject_height, \"SubjectHeight\", train_ds)\n",
        "subject_weight_encoded = encode_numerical_feature(subject_weight, \"SubjectWeight\", train_ds)\n",
        "longitude_encoded      = encode_numerical_feature(subject_height, \"Longitude\", train_ds)\n",
        "latitude_encoded       = encode_numerical_feature(subject_weight, \"Latitude\", train_ds)\n",
        "\n",
        "all_features = layers.concatenate([\n",
        "                                   inout_state_encoded,\n",
        "                                   district_encoded,\n",
        "                                   vehicle_year_encoded,\n",
        "                                   zip_encoded,\n",
        "                                   zone_encoded,\n",
        "                                   vehicle_make_encoded,\n",
        "                                   vehicle_model_encoded,\n",
        "                                   vehicle_style_encoded,\n",
        "                                   vehicle_color_encoded,\n",
        "                                   subject_race_encoded,\n",
        "                                   subject_gender_encoded,\n",
        "                                   hair_color_encoded,\n",
        "                                   time_encoded,\n",
        "                                   subject_age_encoded,\n",
        "                                   subject_height_encoded,\n",
        "                                   subject_weight_encoded,\n",
        "                                   longitude_encoded,\n",
        "                                   latitude_encoded])\n",
        "\n",
        "# build model\n",
        "x = layers.Dense(16, activation=\"relu\")(all_features)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "x = layers.Dense(16, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "x = layers.Dense(16, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "x = layers.Dense(16, activation=\"relu\")(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "x = layers.Dense(16, activation=\"relu\")(x)\n",
        "output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(all_inputs, output)\n",
        "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U_geQsn5ssog",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        },
        "outputId": "610bdd8c-baaf-48b7-fe3a-70e9d5600834"
      },
      "source": [
        "# train model\n",
        "model.fit(train_ds, epochs=20, validation_data=eval_ds)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "6203/6203 [==============================] - 94s 15ms/step - loss: 0.2601 - accuracy: 0.9298 - val_loss: 0.2518 - val_accuracy: 0.9308\n",
            "Epoch 2/20\n",
            "6203/6203 [==============================] - 101s 16ms/step - loss: 0.2523 - accuracy: 0.9306 - val_loss: 0.2516 - val_accuracy: 0.9308\n",
            "Epoch 3/20\n",
            "6203/6203 [==============================] - 104s 17ms/step - loss: 0.2522 - accuracy: 0.9306 - val_loss: 0.2515 - val_accuracy: 0.9308\n",
            "Epoch 4/20\n",
            "6203/6203 [==============================] - 99s 16ms/step - loss: 0.2525 - accuracy: 0.9306 - val_loss: 0.2519 - val_accuracy: 0.9308\n",
            "Epoch 5/20\n",
            "6203/6203 [==============================] - 101s 16ms/step - loss: 0.2521 - accuracy: 0.9306 - val_loss: 0.2516 - val_accuracy: 0.9308\n",
            "Epoch 6/20\n",
            "6203/6203 [==============================] - 97s 16ms/step - loss: 0.2521 - accuracy: 0.9306 - val_loss: 0.2518 - val_accuracy: 0.9308\n",
            "Epoch 7/20\n",
            "6203/6203 [==============================] - 91s 15ms/step - loss: 0.2521 - accuracy: 0.9306 - val_loss: 0.2517 - val_accuracy: 0.9308\n",
            "Epoch 8/20\n",
            "6203/6203 [==============================] - 98s 16ms/step - loss: 0.2521 - accuracy: 0.9306 - val_loss: 0.2516 - val_accuracy: 0.9308\n",
            "Epoch 9/20\n",
            "6203/6203 [==============================] - 99s 16ms/step - loss: 0.2521 - accuracy: 0.9306 - val_loss: 0.2516 - val_accuracy: 0.9308\n",
            "Epoch 10/20\n",
            "6203/6203 [==============================] - 99s 16ms/step - loss: 0.2521 - accuracy: 0.9306 - val_loss: 0.2516 - val_accuracy: 0.9308\n",
            "Epoch 11/20\n",
            "6203/6203 [==============================] - 100s 16ms/step - loss: 0.2521 - accuracy: 0.9306 - val_loss: 0.2515 - val_accuracy: 0.9308\n",
            "Epoch 12/20\n",
            "6203/6203 [==============================] - 99s 16ms/step - loss: 0.2521 - accuracy: 0.9306 - val_loss: 0.2517 - val_accuracy: 0.9308\n",
            "Epoch 13/20\n",
            "6203/6203 [==============================] - 100s 16ms/step - loss: 0.2521 - accuracy: 0.9306 - val_loss: 0.2517 - val_accuracy: 0.9308\n",
            "Epoch 14/20\n",
            "6203/6203 [==============================] - 99s 16ms/step - loss: 0.2521 - accuracy: 0.9306 - val_loss: 0.2515 - val_accuracy: 0.9308\n",
            "Epoch 15/20\n",
            "6203/6203 [==============================] - 100s 16ms/step - loss: 0.2521 - accuracy: 0.9306 - val_loss: 0.2516 - val_accuracy: 0.9308\n",
            "Epoch 16/20\n",
            "6203/6203 [==============================] - 104s 17ms/step - loss: 0.2521 - accuracy: 0.9306 - val_loss: 0.2517 - val_accuracy: 0.9308\n",
            "Epoch 17/20\n",
            "6203/6203 [==============================] - 101s 16ms/step - loss: 0.2521 - accuracy: 0.9306 - val_loss: 0.2516 - val_accuracy: 0.9308\n",
            "Epoch 18/20\n",
            "6203/6203 [==============================] - 100s 16ms/step - loss: 0.2521 - accuracy: 0.9306 - val_loss: 0.2516 - val_accuracy: 0.9308\n",
            "Epoch 19/20\n",
            "6203/6203 [==============================] - 99s 16ms/step - loss: 0.2520 - accuracy: 0.9306 - val_loss: 0.2516 - val_accuracy: 0.9308\n",
            "Epoch 20/20\n",
            "6203/6203 [==============================] - 100s 16ms/step - loss: 0.2521 - accuracy: 0.9306 - val_loss: 0.2518 - val_accuracy: 0.9308\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f4f31bb1940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxGLH6CmHTC2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "b08889bd-cc78-46e4-fd25-195c44c47ed1"
      },
      "source": [
        "# predict from model\n",
        "sample = dict({\n",
        "    \"SubjectDriverLicState\":0,\n",
        "    \"District\":7,\n",
        "    \"VehicleYear\":1997,\n",
        "    \"Zip\":70127,\n",
        "    \"Zone\":\"O\",\n",
        "    \"VehicleMake\":\"CHEVROLET\",\n",
        "    \"VehicleModel\":\"TAHOE\",\n",
        "    \"VehicleStyle\":\"SPORTS UTILITY\",\n",
        "    \"VehicleColor\":\"BLUE\",\n",
        "    \"SubjectRace\":\"BLACK\",\n",
        "    \"SubjectGender\":\"MALE\",\n",
        "    \"SubjectHairColor\":\"Black\",\n",
        "    \"EventDate\":34500,\n",
        "    \"SubjectAge\":21,\n",
        "    \"SubjectHeight\":72,\n",
        "    \"SubjectWeight\":169,\n",
        "    'Longitude':30, \n",
        "    'Latitude':90, \n",
        "})\n",
        "\n",
        "input_dict = {name: tf.convert_to_tensor([value]) for name, value in sample.items()}\n",
        "tf.print(input_dict)\n",
        "model.predict(input_dict)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'District': [7],\n",
            " 'EventDate': [34500],\n",
            " 'Latitude': [90],\n",
            " 'Longitude': [30],\n",
            " 'SubjectAge': [21],\n",
            " 'SubjectDriverLicState': [0],\n",
            " 'SubjectGender': [\"MALE\"],\n",
            " 'SubjectHairColor': [\"Black\"],\n",
            " 'SubjectHeight': [72],\n",
            " 'SubjectRace': [\"BLACK\"],\n",
            " 'SubjectWeight': [169],\n",
            " 'VehicleColor': [\"BLUE\"],\n",
            " 'VehicleMake': [\"CHEVROLET\"],\n",
            " 'VehicleModel': [\"TAHOE\"],\n",
            " 'VehicleStyle': [\"SPORTS UTILITY\"],\n",
            " 'VehicleYear': [1997],\n",
            " 'Zip': [70127],\n",
            " 'Zone': [\"O\"]}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.07523882]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdB5Xh2Q9dc9",
        "colab_type": "text"
      },
      "source": [
        "## **Logistic Regression**\n",
        "\n",
        "Binary Logistic Regression, in which the target variable has only two possible values, e.g., pass/fail or win/lose."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7N_QcET9cH6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "9153f2f6-9280-492e-bd5b-a388be7b1be7"
      },
      "source": [
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import preprocessing\n",
        "from sklearn.preprocessing import OneHotEncoder, normalize\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def encode_cat_features(features, dataframe, encoder):\n",
        "  _dataframe = dataframe.copy()\n",
        "  _dataframe = dataframe[features]\n",
        "  transform_array = encoder.transform(_dataframe).toarray()\n",
        "  return np.transpose(transform_array)\n",
        "\n",
        "def z_score_norm(x, mean, std):\n",
        "  return ( float(x) - float(mean) ) / float(std)\n",
        "\n",
        "def inv_z_score_norm(z, mean, std):\n",
        "  return float(mean) + float(z) * float(std)\n",
        "\n",
        "def encode_norm_features(features, dataframe, mean=None, std=None):\n",
        "  _dataframe = dataframe.copy()\n",
        "  _dataframe = _dataframe[features]\n",
        "  for feature in features:\n",
        "    _mean = mean[feature]\n",
        "    _std  = std[feature]\n",
        "    _dataframe[feature] = _dataframe[feature].apply(lambda x: z_score_norm(x, _mean, _std))\n",
        "  return np.transpose(_dataframe.to_numpy())\n",
        "  \n",
        "# load data\n",
        "police_df = police_interviews.load_dataframe(split=False)\n",
        "police_label_df = police_df.pop(\"StopDescription\")\n",
        "\n",
        "# categorical features\n",
        "categorical = [\"Zone\", \"VehicleMake\", \"VehicleModel\", \"VehicleStyle\",             # string\n",
        "               \"VehicleColor\", \"SubjectRace\", \"SubjectGender\",\"SubjectHairColor\", # string\n",
        "               \"SubjectDriverLicState\", \"District\", \"VehicleYear\", \"Zip\"]         # int\n",
        "\n",
        "numerical = [\"EventDate\", \"SubjectAge\", \"SubjectHeight\", \"SubjectWeight\"]\n",
        "\n",
        "# one-hot encoder\n",
        "# train on the whole set to avoid feature dim errors\n",
        "onehot_encoder = OneHotEncoder(sparse=True)\n",
        "onehot_encoder.fit(police_df[categorical])\n",
        "\n",
        "# split the data into train and test\n",
        "x_train, x_test, y_train, y_test = train_test_split(police_df, police_label_df, test_size=0.2, random_state=1234)\n",
        "\n",
        "# categorical features encoding\n",
        "x_train_cat_encoded = encode_cat_features(categorical, x_train, onehot_encoder)\n",
        "x_test_cat_encoded  = encode_cat_features(categorical, x_test, onehot_encoder)\n",
        "\n",
        "# # numerical features encoding\n",
        "mean = x_train[numerical].mean(axis=0)\n",
        "std  = x_train[numerical].std(axis=0)\n",
        "x_train_num_encoded = encode_norm_features(numerical, x_train, mean, std)\n",
        "x_test_num_encoded  = encode_norm_features(numerical, x_test, mean, std)\n",
        "\n",
        "# # combine categorical and numerical\n",
        "x_train_combined = np.vstack([x_train_cat_encoded, x_train_num_encoded])\n",
        "x_train_combined = np.transpose(x_train_combined)\n",
        "x_test_combined = np.vstack([x_test_cat_encoded, x_test_num_encoded])\n",
        "x_test_combined = np.transpose(x_test_combined)\n",
        "\n",
        "# train model\n",
        "logistic_regression = LogisticRegression(random_state=0, max_iter=1000).fit(x_train_combined, y_train)\n",
        "\n",
        "# # accuracy on test set\n",
        "print(logistic_regression.score(x_train_combined, y_train))\n",
        "print(logistic_regression.score(x_test_combined, y_test))\n",
        "\n",
        "# # predict classes\n",
        "predicted = logistic_regression.predict(x_test_combined[:200, :])\n",
        "expected = y_test[:200].to_numpy()\n",
        "\n",
        "# predict probability\n",
        "logistic_regression.predict_proba(x_test_combined[:200, :])\n",
        "\n",
        "# confusion matrix\n",
        "tn, fp, fn, tp = confusion_matrix(expected, predicted, normalize='pred').ravel()\n",
        "\n",
        "print(\"True Negative:  \", tn) \n",
        "print(\"False Positive: \", fp)\n",
        "print(\"False Negative: \", fn)\n",
        "print(\"True Prositive: \", tp)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.9306590997033406\n",
            "0.9308138784986457\n",
            "True Negative:   0.91\n",
            "False Positive:  0.0\n",
            "False Negative:  0.09\n",
            "True Prositive:  0.0\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}