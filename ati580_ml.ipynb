{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ati580_ml.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOkVvSVBp0SzpilHft79Zvt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uninstallit/ati580_final_project/blob/edvin-1/ati580_ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRsRJpBpCyr3",
        "colab_type": "text"
      },
      "source": [
        "**Machine Learning portion of ATI580 Final Project**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXeaEC9UDlom",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a6c50042-d754-4439-abbd-f7bdcd460e60"
      },
      "source": [
        "pip install dnspython"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dnspython in /usr/local/lib/python3.6/dist-packages (1.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "semGogYxCjNT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "877f574b-c4eb-41f5-b1e2-a8b3a0bab0fd"
      },
      "source": [
        "import pymongo\n",
        "\n",
        "mdb_client = pymongo.MongoClient(\"mongodb+srv://mdbUser:ati580@ati580-cluster.s5t5z.gcp.mongodb.net/POLICE_DATABASE?retryWrites=true&w=majority\")\n",
        "mdb_database   = mdb_client['POLICE_DATABASE'] \n",
        "mdb_collection = mdb_database['POLICE_INTERVIEWS']\n",
        "\n",
        "records = mdb_collection.find({})\n",
        "print(records[0].keys()) \n",
        "\n",
        "mdb_collection.count_documents({})"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['_id', 'FieldInterviewID', 'NOPD_Item', 'EventDate', 'District', 'Zone', 'OfficerAssignment', 'StopDescription', 'ActionsTaken', 'VehicleYear', 'VehicleMake', 'VehicleModel', 'VehicleStyle', 'VehicleColor', 'SubjectID', 'SubjectRace', 'SubjectGender', 'SubjectAge', 'SubjectHasPhotoID', 'SubjectHeight', 'SubjectWeight', 'SubjectEyeColor', 'SubjectHairColor', 'SubjectDriverLicState', 'CreatedDateTime', 'LastModifiedDateTime', 'Longitude', 'Latitude', 'Zip', 'BlockAddress'])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "593893"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z2DDDhpwoYnz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "7a63c966-4a95-4d45-938d-ee07b6e9bc9a"
      },
      "source": [
        "from pandas import DataFrame\n",
        "\n",
        "test = mdb_collection.distinct('StopDescription')\n",
        "test_df = DataFrame(list(test))\n",
        "print(test_df)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test length:  10\n",
            "                        0\n",
            "0        CALL FOR SERVICE\n",
            "1         CITIZEN CONTACT\n",
            "2      CRIMINAL VIOLATION\n",
            "3            FLAGGED DOWN\n",
            "4      JUVENILE VIOLATION\n",
            "5                   OTHER\n",
            "6  PRESENT AT CRIME SCENE\n",
            "7          SUSPECT PERSON\n",
            "8         SUSPECT VEHICLE\n",
            "9       TRAFFIC VIOLATION\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yR3j6WP06CF1",
        "colab_type": "text"
      },
      "source": [
        "## **Structured data classification**\n",
        "- Data includes both numerical and categorical features. \n",
        "- Use Keras preprocessing layers to normalize the numerical features and vectorize the categorical ones.\n",
        "- Source: [Keras | Code examples](https://keras.io/examples/structured_data/structured_data_classification_from_scratch/)\n",
        "\n",
        "## **Column Description**\n",
        "- SubjectGender   (Categorical)\n",
        "- SubjectAge.     (Numerical)\n",
        "- StopDescription (Categorical)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rxvMtBdv8Vxk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 308
        },
        "outputId": "3647657d-f799-4fda-98c3-744c09be5609"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# prepare the data \n",
        "cursor = mdb_collection.find({\n",
        "    \"$and\":[{\"SubjectGender\":{\"$exists\": True}}, \n",
        "            {\"SubjectGender\":{\"$ne\": \"\"}},\n",
        "            {\"SubjectGender\":{\"$ne\": None}},\n",
        "            {\"SubjectAge\":{\"$exists\": True}}, \n",
        "            {\"SubjectAge\":{\"$ne\": \"\"}},\n",
        "            {\"SubjectAge\":{\"$ne\": None}},\n",
        "            {\"StopDescription\":{\"$exists\": True}}, \n",
        "            {\"StopDescription\":{\"$ne\": \"\"}},\n",
        "            {\"StopDescription\":{\"$ne\": None}} ]}, \n",
        "            { \"_id\":0, \"SubjectGender\" : 1 , \"SubjectAge\" : 1 , \"StopDescription\" : 1})\n",
        "\n",
        "subject_df = pd.DataFrame(list(cursor))\n",
        "print(\"subject dataframe shape: {} \\n\".format(subject_df.shape))\n",
        "print(subject_df.head())\n",
        "\n",
        "reasonable_suspicion = {\"CALL FOR SERVICE\":1,\n",
        "                        \"CITIZEN CONTACT\":1,\n",
        "                        \"FLAGGED DOWN\":1,\n",
        "                        \"OTHER\":1,\n",
        "                        \"PRESENT AT CRIME SCENE\":1,\n",
        "                        \"SUSPECT PERSON\":1,\n",
        "                        \"SUSPECT VEHICLE\":1}\n",
        "\n",
        "probable_cause_list = {\"CRIMINAL VIOLATION\":0, \n",
        "                       \"JUVENILE VIOLATION\":0, \n",
        "                       \"TRAFFIC VIOLATION\":0}\n",
        "\n",
        "subject_df[\"StopDescription\"] = subject_df[\"StopDescription\"].replace(reasonable_suspicion)\n",
        "subject_df[\"StopDescription\"] = subject_df[\"StopDescription\"].replace(probable_cause_list)\n",
        "\n",
        "print(subject_df.head())\n",
        "\n",
        "# split the data into a training and validation set\n",
        "eval_subject_df  = subject_df.sample(frac=0.2, random_state=1337)\n",
        "train_subject_df = subject_df.drop(eval_subject_df.index)\n",
        "print(\"Using %d samples for training and %d for validation\" % (len(train_subject_df), len(eval_subject_df)))\n",
        "\n",
        "# convert dataframe_to_dataset \n",
        "def dataframe_to_dataset(dataframe, output_name):\n",
        "    _dataframe = dataframe.copy()\n",
        "    _labels    = _dataframe.pop(output_name)\n",
        "    _dataset   = tf.data.Dataset.from_tensor_slices((dict(_dataframe), _labels))\n",
        "    _dataset   = _dataset.shuffle(buffer_size=len(_dataframe))\n",
        "    return _dataset\n",
        "\n",
        "train_subject_ds = dataframe_to_dataset(train_subject_df, \"StopDescription\")\n",
        "eval_subject_ds  = dataframe_to_dataset(eval_subject_df, \"StopDescription\")\n",
        "\n",
        "train_subject_ds = train_subject_ds.batch(32)\n",
        "eval_subject_ds  = eval_subject_ds.batch(32)\n",
        "\n",
        "print(\"dataset preparation done\")"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "subject dataframe shape: (576132, 3) \n",
            "\n",
            "     StopDescription SubjectGender  SubjectAge\n",
            "0  TRAFFIC VIOLATION        FEMALE        26.0\n",
            "1   CALL FOR SERVICE          MALE        17.0\n",
            "2   CALL FOR SERVICE          MALE        18.0\n",
            "3   CALL FOR SERVICE          MALE        18.0\n",
            "4   CALL FOR SERVICE          MALE        30.0\n",
            "   StopDescription SubjectGender  SubjectAge\n",
            "0                0        FEMALE        26.0\n",
            "1                1          MALE        17.0\n",
            "2                1          MALE        18.0\n",
            "3                1          MALE        18.0\n",
            "4                1          MALE        30.0\n",
            "Using 460906 samples for training and 115226 for validation\n",
            "dataset preparation done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "estOeFNiezMH",
        "colab_type": "text"
      },
      "source": [
        "## **one-hot Encoding of features**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKQ251LZeyi1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "d0467dd1-7e57-45b9-8bf4-76ccbfcb9a3a"
      },
      "source": [
        "from tensorflow.keras.layers.experimental.preprocessing import Normalization\n",
        "from tensorflow.keras.layers.experimental.preprocessing import CategoryEncoding\n",
        "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
        "\n",
        "\n",
        "def encode_numerical_feature(feature, name, dataset):\n",
        "    # Create a Normalization layer for our feature\n",
        "    normalizer = Normalization()\n",
        "    # Prepare a Dataset that only yields our feature\n",
        "    feature_ds = dataset.map(lambda x, y: x[name])\n",
        "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
        "    # Learn the statistics of the data\n",
        "    normalizer.adapt(feature_ds)\n",
        "    # Normalize the input feature\n",
        "    encoded_feature = normalizer(feature)\n",
        "    return encoded_feature\n",
        "\n",
        "def encode_string_categorical_feature(feature, name, dataset):\n",
        "    # Create a StringLookup layer which will turn strings into integer indices\n",
        "    index = StringLookup()\n",
        "    # Prepare a Dataset that only yields our feature\n",
        "    feature_ds = dataset.map(lambda x, y: x[name])\n",
        "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
        "    # Learn the set of possible string values and assign them a fixed integer index\n",
        "    index.adapt(feature_ds)\n",
        "    # Turn the string input into integer indices\n",
        "    encoded_feature = index(feature)\n",
        "    # Create a CategoryEncoding for our integer indices\n",
        "    encoder = CategoryEncoding(output_mode=\"binary\")\n",
        "    # Prepare a dataset of indices\n",
        "    feature_ds = feature_ds.map(index)\n",
        "    # Learn the space of possible indices\n",
        "    encoder.adapt(feature_ds)\n",
        "    # Apply one-hot encoding to our indices\n",
        "    encoded_feature = encoder(encoded_feature)\n",
        "    return encoded_feature\n",
        "\n",
        "def encode_integer_categorical_feature(feature, name, dataset):\n",
        "    # Create a CategoryEncoding for our integer indices\n",
        "    encoder = CategoryEncoding(output_mode=\"binary\")\n",
        "    # Prepare a Dataset that only yields our feature\n",
        "    feature_ds = dataset.map(lambda x, y: x[name])\n",
        "    feature_ds = feature_ds.map(lambda x: tf.expand_dims(x, -1))\n",
        "    # Learn the space of possible indices\n",
        "    encoder.adapt(feature_ds)\n",
        "    # Apply one-hot encoding to our indices\n",
        "    encoded_feature = encoder(feature)\n",
        "    return encoded_feature"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-110-cbe85cbeb79e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCategoryEncoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mStringLookup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'CategoryEncoding'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s9OAOem0mAye",
        "colab_type": "text"
      },
      "source": [
        "## **Build the model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZC92GRZJmEUd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SubjectGender\n",
        "SubjectAge\n",
        "StopDescription\n",
        "\n",
        "\n",
        "# Categorical features encoded as integers\n",
        "# stop_description = keras.Input(shape=(1,), name=\"StopDescription\", dtype=\"int64\")\n",
        "\n",
        "# Categorical feature encoded as string\n",
        "subject_gender = keras.Input(shape=(1,), name=\"SubjectGender\", dtype=\"string\")\n",
        "\n",
        "# Numerical features\n",
        "subject_age = keras.Input(shape=(1,), name=\"SubjectAge\")\n",
        "\n",
        "all_inputs = [subject_gender, subject_age]\n",
        "\n",
        "# Integer categorical features\n",
        "\n",
        "# String categorical features\n",
        "subject_gender_encoded = encode_string_categorical_feature(subject_gender, \"SubjectGender\", train_subject_ds)\n",
        "\n",
        "# Numerical features\n",
        "subject_age_encoded = encode_numerical_feature(subject_age, \"SubjectAge\", train_subject_ds)\n",
        "\n",
        "all_features = layers.concatenate([subject_gender_encoded, subject_age_encoded])\n",
        "\n",
        "x = layers.Dense(32, activation=\"relu\")(all_features)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "output = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "model = keras.Model(all_inputs, output)\n",
        "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}